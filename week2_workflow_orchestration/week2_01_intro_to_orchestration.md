# Intro to Orchestration
- We will be running **Mage** and Postgres *within* a Docker environment
- We will be downloading (extracting) NYC Taxi data via an API, transforming it a little bit (cleaning, some partitioning, etc.), loading it into Postgres and Google Cloud Storage as a data lake via Mage, doing some more transformations (via SQL, pandas, Apache Arrow, etc.), and finally load it into Google BigQuery in a data warehouse (again via Mage)


## What is Orchestration?
- A lot of data engineering is ETL/ELT between various sources and destinations
- **Orchestration** is a process of *dependency management*, facilitated through *automation*
    - Automation is key here
    - As engineers, the idea is to *minimize manual labor* by automating as many processes as possible
- The **data orchestrator** manages *scheduling*, *triggering*, *monitoring*, and even *resource allocation*
- Every **workflow** requires *sequential* steps, in a particular order, since downstream processes will depend on upstream processes
- We will refer to "steps" as **tasks** (or "blocks" in Mage) and to workflows as **pipelines** or **DAGs  (Directed Acyclic Graphs)**
- Orchestration is one of the **undercurrents** of the **Data Engineering Lifecycle** from the *Fundamentals of Data Engineering* book (https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/)
    - By "undercurrent", we mean that it happens throughout the *entire* lifecycle, and it is key to building data engineering pipelines
- So, it's important to have a good orchestrator that fits your specific use case(s) well (there is no "perfect" solution for all use cases)
- So, *what does a good solution look like?*
    - A good orchestrator handles:
        - Workflow management
            - Define schedules, manage workflows efficiently, ensure tasks are executed in the right order, manages dependencies, etc.
        - Automation
            - Important since we're trying to automate as much as possible
        - Error handling
            - Things *will* break, so orchestrators need built-in solutions for: handling errors, conditional logic, branching, and retrying failed tasks
        - Recovery
            - Again, things *will* break, so we need an efficient way to **backfill** or recover any lost/missing data
        - Monitoring and alerting
            - Notifications should be able to be sent if something fails or is being retried in a pipeline
        - Resource optimization
            - Ideally, an orchestrator is optimizing the best route for where jobs are executed
        - Observability
            - A very important part of data engineering is having visibility into *every* part of a data pipeline
        - Debugging
            - This is a part of observability, and this should be easily done via the orchestrator
        - Compliance/Auditing
            - Because the orchestrator should have observability and debugging functionalities (and because it's an undercurrent), it should help with compliance and auditing 
    - Also, a good orchestrator *prioritizes* **the developer experience**:
        - Flow state
            - Don't want to be switching between a lot of different services/tools
        - Feedback loops
            - Want to be able to iterate quickly, fail fast, build good products and get tangible feedback quickly
        - Cognitive load
            - How much do you need to know to do your job? Was the day of work effortless or headache-inducing?
- So, a good orchestrator handles all the data engineering tasks mentioned above but also facilitates rapid and almost seamless development of efficient pipelines