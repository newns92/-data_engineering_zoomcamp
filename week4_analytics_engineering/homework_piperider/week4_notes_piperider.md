# PipeRider: Maximizing Confidence in Your Data Model Changes with dbt and PipeRider
- https://www.youtube.com/watch?v=O-tyUOQccSs&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=41
- https://docs.piperider.io/
- PipeRider allows better PR's and also gives HTML reports about data, as well as comparison reports and dbt metrics
- *Why do we need a better PR process?*
    - There are large and complex dbt projects out there
    - There are also large-scale datasets that are hard to understand
    - When making model changes:
        - It's difficult to assess change impact
        - Errors may make it into production
- Current methods of checking data:
    - Eyeballing the data (or running "random" queries)
    - BI tools 
        - Not ideal
        - Data should be ready by the time it reaches BI tools
        - No easy way to compare DEV and PROD data
    - dbt tests
        - Powerful, but you'd need to actually write the tests
        - Signal-to-noise issue with many tests
- **PipeRider**: an open-source data profiling toolkit with "improved code reviews for data projects" in dbt (https://github.com/InfuseAI/piperider)
    - Has an open-source CLI tool
        - Data profiler,
        - HTML data profile reports
        - Data profile comparisons
        - Multiple data connectors available (modern data stack)
        - Integration with dbt
        - Data assertions
    - Has a Cloud option (BETA as of 02/2023)
        - Upload and view reports
        - Compare reports online
        - Share reports
        - Currently (02/2023) free to try: https://cloud.piperider.io/signin
- Why use a **Data Profiler**?
    - It's a Swiss-army knife tool for data
    - Data exploration = understand data
    - Data quality analysis (completeness of data, etc.)
    - Discover hidden data (outliers, odd values, etc.)
    - Suitable for large datasets
    - Easy to digest and share a profile report (without needing to first build a dashboard)
- **PipeRider + dbt**
    - *PipeRider automatically detects connections settings for dbt projects*
        - dbt not required at all
        - PipeRider can connect to data sources directly
    - Available connectors/supported data sources: BigQuery, DuckDB, Postgres, Redshift, Snowflake, CSV, Parquet, SQLite (default)
- **Using PipeRider in a dbt project**
    - 1\) Developing and testing data models locally
        - Engineers creating and modifying models locally
    - 2\) Deploying model changes
        - Reviewing changes submitted by PR's to existing projects
        - Taps into the version controlled nature of dbt projects
        - Automated as part of the CI process
- PipeRider Process
    - You build your dbt models > Profile them with PipeRider > Generate PipeRider report(s) > PipeRider Compare > (Do any potential model changes) > Comparison report and summary
    - Basic (local) workflow
        - Create a git branch
        - Install PipeRider
        - Initialize PipeRider
        - `dbt build`
        - Run PipeRider (initial report)
        - Update data models
        - `dbt build`
        - Run PipeRider (2nd report)
        - Compare reports
        - Create PR with comparison summary

## DuckDB
- First, install DuckDB in the `zoom` Anaconda environment via `pip install duckdb==0.7.1`
- When to use DuckDB
    - Processing and storing tabular datasets (e.g. from CSV or Parquet files)
    - Interactive data analysis (e.g. Joining & aggregate multiple large tables)
    - Concurrent large changes, to multiple large tables (e.g. appending rows, adding/removing/updating columns)
    - Large result set transfer to client
- Download the DuckDB database file: `wget https://dtc-workshop.s3.ap-northeast-1.amazonaws.com/nyc_taxi.duckdb`
- Copy in all dirs/files from `https://github.com/InfuseAI/taxi_rides_ny_duckdb`
- Install the neccessary dbt packages and PipeRider: `pip install dbt-core dbt-duckdb piperider[duckdb]`
- In Git Bash, Create a new branch to work on: `git switch -c duckdb`
- In an Anaconda prompt, in the `zoom` environment (after making sure we're in the `duckdb` git branch), install dbt deps and build dbt models via: `dbt deps`, then `dbt build`
- Should only get 1 WARNING, which we don't have to worry about right now

## PipeRider
Initialize PipeRider via `piperider init` to create `.piperider/` and its subdirectories and files
    - PipeRider will automatically detect the data/project settings in the CLI
- PipeRider stores all its files in the `.piperider/` dir (does not touch or affect the data)
- In `.piperider/config.yml`, enable/uncomment the code under `profiler:` and change `duplicateRows:` to `True`, and re-comment out `limit: 1000000`
- Now, we go back to the CLI to actually use PipeRider and modify the project
- First, we create the first data report to act as our baseline for our comparisons via `piperider run`
- See that this creates a report in `\.piperider\outputs`