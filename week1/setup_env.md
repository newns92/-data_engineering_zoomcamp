# Docker: 
- https://docs.docker.com/desktop/install/windows-install/
- https://medium.com/@verazabeida/zoomcamp-2023-week-1-f4f94cb360ae
- Check for running containers with `docker ps`
# Docker Compose
- way of running multiple Docker images
# Postgres
- `winpty docker run -it -e POSTGRES_USER="root" -e POSTGRES_PASSWORD="root" -e POSTGRES_DB="ny_taxi" -v C://Users//[rest of path]//ny_taxi_postgres_data:/var/lib/postgresql/data:rw -p 5432:5432 postgres:13`
- Install a client for Postgres: `conda install -c conda-forge pgcli`
- Add to the system path: `C:\ProgramData\Miniconda3\Scripts`
- Test with `pgcli --help`
- Install keyring with `conda install keyring`
- Connect to Postgres IN A SEPARATE COMMAND WINDOW with `pgcli -h localhost -p 5432 -u root -d ny_taxi` where -d = database, -u = user, -p = port, -h = host
- Test by checking for tables with `\dt` and also doing `SELECT 1`
- Exit with CTRL+D
- See `load_data.py` to load in data
- Test with `SELECT COUNT(*) FROM yellow_taxi_data` (1369765) and `SELECT COUNT(*) FROM zones` (265)
- Test again with `SELECT MAX(tpep_pickup_datetime), MIN(tpep_pickup_datetime), max(total_amount) FROM yellow_taxi_data`
    - should get `2021-02-22 16:52:16`, `2008-12-31 23:05:16`, and `7661.28`
# pgAdmin
- Itâ€™s not convenient to use `pgcli` for data exploration and querying
- `pgAdmin` - the standard web-based GUI for Postgres data exploration and querying (both local and remote servers)
- Don't need to install it since we have Docker, so we can just pull an image that contains this tool
- Google `pgadmin docker`, select the first link (to the container)
- Click the "instructions on Dockerhub" link
- Will have to use `docker pull dpage/pgadmin4`
- Full command: `docker run -it -e PGADMIN_DEFAULT_EMAIL="admin@admin.com" -e PGADMIN_DEFAULT_PASSWORD="root" -p 8080:80 dpage/pgadmin4`
    - notice we port 8080 on our machine to 80 in the container
- Once you see `Listening at: http://[::]:80 (1)`, open browser and go to `http://localhost:8080/` to see the landing page for pgAdmin
- Sign in with the credentials from the command above
- Create a new server: `Local Docker` by right-clicking on "Servers" and hit "Register" --> "Server"
- Need to specify the host address in the "Connection" tab, which should be `localhost`, port is `5432`, username and password is `root`
- Will see an error of unable to connect to this server because we're running pgAdmin inside of a container, and this is trying to find postgres localhost within this container, though it is in *another* container
# Docker Networks
- In other words, this pgAdmin Docker container cannot access the Postgres container, and we need to **link them** via a **Docker network**
- Shut down both containers
- Create a network with:
    - 1) Create the network itself with `docker network create pg-network`
    - 2) Run Postgres, specifying it should be run in the network, with `winpty docker run -it -e POSTGRES_USER="root" -e POSTGRES_PASSWORD="root" -e POSTGRES_DB="ny_taxi" -v C://Users//nimz//Dropbox//de_zoomcamp//week1//ny_taxi_postgres_data:/var/lib/postgresql/data:rw -p 5432:5432 --name pgdatabase --network=pg-network postgres:13`
        - a) Check that the data has persisted with `SELECT COUNT(*) FROM yellow_taxi_data` (1369765)
    - 3) In a separate command window, run pgAdmin, specifying it should be run in the network, with `docker run -it -e PGADMIN_DEFAULT_EMAIL="admin@admin.com" -e PGADMIN_DEFAULT_PASSWORD="root" -p 8080:80 --name pgadmin --network=pg-network dpage/pgadmin4`
    - *Notice the commands are similar but we added `--name` and `--net` arguments*
- Create the server again in same manner as above, BUT host name/address should be `pgdatabase`, port is `5432`, username and password is `root`
- Should have been able to connect and see "Local Docker" under "Servers" on the left of pgAdmin
- Can see our tables under "Local Docker" --> "Databases (2)" --> "ny_taxi" --> "Schemas (1)" --> "Tables (2)"
- Can view data with `SELECT * FROM public.yellow_taxi_data LIMIT 100`
- Can write the same query by going to "Tools" --> "Query Tool"
- Next, we will put those two `docker run -it` commands into a single YAML file to run both containters with one terminal via `docker compose`
# Docker Compose
- Docker Compose lets us run multiple containers and link them in a network
- Docker compose lets us codify the Docker shell commands into a YAML file so that we don't have to remember the correct sequence to run network commands, + all of the flags and environment variables
- Create the `docker-compose.yml` file
    - Don't need to write the full path for the volumes\
    - Docker provides `restart` policies to control whether your containers start automatically when they exit, or when Docker restarts, and these that linked containers are started in the correct order
    - The containers automatically become part of a network, so we don't have to specify it here
- Check that nothing is running with `docker ps`
- Then run `docker-compose up`
    - will have to create a new instance of the database server in the browser since we didn't do volumes mappings
- Or run in detached mode with `docker-compose up -d` to get control of the terminal back once things are spun up
- Open browser and go to `http://localhost:8080/` to see the landing page for pgAdmin
- Shut it down with `docker-compose down`
- **To make pgAdmin configuration persistent**, create a folder `data_pgadmin`
    - Change its permission potentially (on Linux, `sudo chown 5050:5050 data_pgadmin`)
    - Mount it to the `/var/lib/pgadmin` folder under `volumes` in the `pgadmin1` service in `docker-compose.yml`
# Dockerize Data Load Script
- If you have a Jupyter notebook, convert it to a Python script via `jupyter nbconvert --to=script [notebook name].ipynb`
- Update the `load_data.py` script
- Drop the 2 tables in pgAdmin
- Run `load_data.py` with all args to recreate them
- Create a `.bash_profile` file in `C:\Users\[Username]` and add the line `alias python='winpty /c/ProgramData/Miniconda3/python.exe'` to use the right python version in Git Bash
- Make sure the Docker container for Postgres is running (via `docker run -it` or `docker-compose up`)
- Run the command
- Double-check that the tables were remade
- **Now, to put this into Docker**
- Add `sqlalchemy` and `psycopg2` to the `pip install pandas` line in `Dockerfile` file
- Add `RUN apt-get install wget` to the `Dockerfile` file
- Change all instances of `pipeline.py` to `load_data.py`
- In `docker-compose.yml`, add the `networks` to each service, then add the `network`
- Run `docker build -t taxi_ingest:v001 .`
- Once complete, run the same command as manually running the Python script to ingest data, but via Docker starting with `docker run -it --network=pg-network taxi_ingest:v001`
# SQL Refresher
- Run the network via `docker-compose.yml` via `docker-compose up -d` in Git Bash or command prompt
# GCP 
- Create account and get the GCP free trial if needed
- Create a new project and note the Project ID
- Go to "IAM" (Identity and Access Management) --> Service Accounts
    - **Service account** = an account with limited permissions that is assigned to a service (ex: a server or VM)
        - Allows us to create a set of credentials that does not have full access to the owner/admin account
- Create a service account: "de_zoomcamp_user"
- Grant access as "Basic" --> "Viewer"
    - We will fine tune the permissions in a later step
- We do not need to "grant users access to this service account"
    - But this is useful in a PROD environment where it may be useful for multiple users to share the same permissions
- To create a key, click the three dots under "Actions" --> "Manage keys"
- Click "Add key" --> "Create new key" --> "JSON"
    - This downloads a *private* key JSON File
    - Do NOT save it in git
- Install the Google Cloud CL (https://cloud.google.com/sdk/docs/install-sdk)
- **If you uncheck bundled Python**:
    - In Google Cloud SDK Shell, check `python -c "import os, sys; print(os.path.dirname(sys.executable))"` to see where Python is installed
    - Check the environment variable in Git Bash via `printenv CLOUDSDK_PYTHON`
    - Use a python (Miniconda) you have installed in a special location via `export CLOUDSDK_PYTHON=C:\ProgramData\Miniconda3\python.exe`
    - Also attempt to set it up via Environment Variables for User and System
    - Re-run the SDK installer
    - Test in Git Bash via `gcloud -h`
- Set environment variable to point to your downloaded GCP keys via `export GOOGLE_APPLICATION_CREDENTIALS="<path/to/your/service-account-authkeys>.json"`
    - This is how we authenticate our other resources (OAuth)
- Refresh token/session, and verify authentication via `gcloud auth application-default login`
    - The browser will open up, so choose the right email and then click "Allow" to see the "You are now authenticated with the gcloud CLI!" webpage
- Next we will set up 2 resources in the Google environment (Google Cloud Storage data lake and BigQuery data warehouse)
    - Cloud storage is a bucket in our GCP environment where we can store data in flat files
    - This data lake is where we will store data in a more organized fashion
    - In our BigQuery data warehouse, our data will be in a more structured format (Fact and Dimension tables)
- 1st, we will add more permissions to our service account
    - Click on "IAM" on the left
    - Click "edit principal" pencil on the right for the user account we just created
    - Add "Storage Admin" to create and the GCP data lake
        - Allows us to create and modify buckets and packets (Terraform) and files
        - In PROD, we'd actually create *custom* roles to limit user access to a particular bucket/certain resources
            - We'd create separate service accounts for Terraform, for the data pipeline, etc. (In this course we are only making *one* for simplicity's sake)
    - Add "Storage Object Admin" to add/control things within our bucket/data lake
    - Add "BigQuery Admin"
    - Click "Save"
- Next, we need to enable API's
    - When the local environment interacts with the cloud enviroment, it does not interact *directly* with the resource
    - These API's are the form of communication
    - We have 2 API's for the IAM itself
        - Click on the 3 bars to the left of "Google Cloud" in the upper-left
        - Click "API's and Services" --> "Library" --> Search for "Identity and Access Management"
        - Choose "Identity and Access Management (IAM) API" --> "Enable"
        - Go back to the library and search for "IAM"
        - Choose "IAM Service Account Credentials API" (which may already be enabled)
# Terraform
- Open source tool for provisioning infrastructure resources
- Supports DevOps best practices for **change management**
- **Infrastructure as Code (IaC)** - check in cloud infrastucture configuration to version control
    - Framework that allows us to build, change, and manage infrastructure in a safe, consistent, and repeatable way by defining resource configurations that you can version, reuse, and share
    - Can manage infrastructure with `config` files alone rather than via a GUI
- Advantages
    - Infrastructure lifecycle management
    - Version control commits
    - Very useful for stack-based deployments, and with cloud providers such as AWS, GCP, Azure, K8Sâ€¦
    - State-based approach to track resource changes throughout deployments
- Install chocolatey
    - Run `Get-ExecutionPolicy` in Windows Powershell. If it returns `Restricted`, then run `Set-ExecutionPolicy AllSigned` or `Set-ExecutionPolicy Bypass -Scope Process`
    - Run `Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))`
- Install with `choco install terraform`
- Have some required files:
    - `.teraform-version` = defines the version of Terraform
    - `main.tf` = defines the resources needed
    - `variables.tf` = defines runtime arguments that will be passed to Terraform
        - Default values can be defined in which case a run time argument is not required
- Execution steps
    - `terraform init:`
        - Initializes & configures the backend, installs plugins/providers, & checks out an existing configuration from a version control
    - `terraform plan:`
        - Matches/previews local changes against a remote state, and proposes an Execution Plan.
        - Describes the actions Terraform will take to create an infrastructure that will to match with our configuration
        - Does not actually create the resources
    - `terraform apply:`
        - Asks for approval to the proposed plan from the previous command, and applies changes to the cloud
        - Actually creates the resources
    - `terraform destroy`
        - Removes your stack from the Cloud
- After getting the requires files, run `terraform init` in the `terraform` directory containing said files
- Run `ls -la` in Git Bash to see some new files, like `.terraform`, which is like any package installer like `pip`
- Run `cd .terraform` and run `ls` to see its files
- Go back with `cd ..` and run `terraform plan`
- If we did not enter our GCP project ID into `variables.tf`, enter it here when prompted
    - Or just run `terraform plan -var="project=<your-gcp-project-id>"`
- Run `terraform apply` and enter the GCP project ID again
    - Or just run `terraform apply -var="project=<your-gcp-project-id>"`
- Enter `yes` and should see `Apply complete! Resources: 2 added, 0 changed, 0 destroyed`
- Go to "Cloud Storage" on the left of the project page in the browser, and should see our new data lake
- Go to "Big Query" on the left of the project page in the browser, and should see our new instance in the "Explorer" tab on the left
- Delete this infrastructure after our work as to avoid costs on any running services via `terraform destroy`
    - If we did not enter our GCP project ID into `variables.tf`, enter it here when prompted
    - Should see `Destroy complete! Resources: 2 destroyed.`
    - Afterwards, should see nothing after going to "Cloud Storage" or "BigQuery" on the left-hand side of GCP
