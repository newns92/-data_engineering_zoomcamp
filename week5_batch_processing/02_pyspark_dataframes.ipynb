{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32210e2a",
   "metadata": {},
   "source": [
    "# *Make sure you have ran all of `01_pyspark_intro.ipynb` to write the partitioned data to the local data directory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13a92c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 08A3-CF2D\n",
      "\n",
      " Directory of C:\\Users\\nimz\\Documents\\de_zoomcamp\\week5_batch_processing\\data\\fhvhv\\2021\\01\n",
      "\n",
      "02/27/2024  06:51 PM    <DIR>          .\n",
      "02/27/2024  06:51 PM    <DIR>          ..\n",
      "02/27/2024  06:51 PM            71,484 .part-00000-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,468 .part-00001-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,476 .part-00002-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,480 .part-00003-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,480 .part-00004-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,484 .part-00005-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,444 .part-00006-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,480 .part-00007-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,464 .part-00008-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,468 .part-00009-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,456 .part-00010-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,464 .part-00011-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,500 .part-00012-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,476 .part-00013-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,512 .part-00014-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,468 .part-00015-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,524 .part-00016-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,488 .part-00017-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,508 .part-00018-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,472 .part-00019-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,492 .part-00020-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,444 .part-00021-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,460 .part-00022-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM            71,496 .part-00023-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet.crc\n",
      "02/27/2024  06:51 PM                 8 ._SUCCESS.crc\n",
      "02/27/2024  06:51 PM         9,148,662 part-00000-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,146,467 part-00001-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,147,559 part-00002-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,148,033 part-00003-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,148,117 part-00004-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,148,532 part-00005-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,143,335 part-00006-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,148,092 part-00007-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,146,261 part-00008-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,146,732 part-00009-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,145,061 part-00010-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,146,075 part-00011-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,150,733 part-00012-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,147,636 part-00013-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,152,105 part-00014-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,146,498 part-00015-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,153,804 part-00016-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,149,358 part-00017-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,151,965 part-00018-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,147,383 part-00019-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,149,612 part-00020-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,143,484 part-00021-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,145,646 part-00022-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM         9,150,453 part-00023-41925c18-80ed-4acf-be10-12b01f6fd8d4-c000.snappy.parquet\n",
      "02/27/2024  06:51 PM                 0 _SUCCESS\n",
      "              50 File(s)    221,267,099 bytes\n",
      "               2 Dir(s)  241,448,771,584 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir .\\data\\fhvhv\\2021\\01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "595c7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Instantiate a Spark session, an object that we use to interact with Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaddcd3",
   "metadata": {},
   "source": [
    "#### You should now see a Spark UI available at http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99de2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the partitioned files into a Spark dataframe\n",
    "df_spark = spark.read.parquet('./data/fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c2bfb",
   "metadata": {},
   "source": [
    "#### Note that Parquet files are smaller because they *know* the schema and use more efficient ways of compressing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66f78dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at the DataFrame schema\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad09ee1",
   "metadata": {},
   "source": [
    "# What can we do with Spark DataFrames?\n",
    "\n",
    "**We can do the usual stuff we that do with pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a0a7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only specific columns\n",
    "df_spark.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c2e5724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-03 15:59:58|2021-01-03 16:13:50|         144|         261|\n",
      "|2021-01-01 14:39:29|2021-01-01 14:59:45|         148|          68|\n",
      "|2021-01-01 07:25:16|2021-01-01 07:50:46|          61|          76|\n",
      "|2021-01-02 01:05:28|2021-01-02 01:11:40|          42|          42|\n",
      "|2021-01-02 13:01:44|2021-01-02 13:25:23|         155|         177|\n",
      "|2021-01-01 05:51:46|2021-01-01 06:03:24|          49|         177|\n",
      "|2021-01-01 02:12:08|2021-01-01 02:19:49|          94|         174|\n",
      "|2021-01-01 02:17:17|2021-01-01 02:34:03|          42|           4|\n",
      "|2021-01-01 01:05:04|2021-01-01 01:17:42|         231|         265|\n",
      "|2021-01-03 01:05:38|2021-01-03 01:09:14|         229|         141|\n",
      "|2021-01-03 00:37:31|2021-01-03 01:01:18|         179|          14|\n",
      "|2021-01-01 17:23:04|2021-01-01 17:44:37|          76|          91|\n",
      "|2021-01-01 21:10:25|2021-01-01 21:30:13|         263|          69|\n",
      "|2021-01-01 14:22:54|2021-01-01 14:46:51|         229|          73|\n",
      "|2021-01-01 15:50:41|2021-01-01 15:56:03|         130|         130|\n",
      "|2021-01-01 20:19:48|2021-01-01 20:31:48|         241|         254|\n",
      "|2021-01-01 21:56:48|2021-01-01 22:04:26|         144|         231|\n",
      "|2021-01-01 19:54:28|2021-01-01 19:59:41|         159|          69|\n",
      "|2021-01-03 10:14:11|2021-01-03 10:23:06|          29|          21|\n",
      "|2021-01-02 17:02:12|2021-01-02 17:22:48|          88|         265|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do filtering\n",
    "df_spark.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "    .filter(df_spark.hvfhs_license_num == 'HV0003') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc0c5f",
   "metadata": {},
   "source": [
    "#### You should see a new job in the Spark UI for this command above, but NOT for the `.select()` command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf36ff7",
   "metadata": {},
   "source": [
    "**Note that `.partition()` and `.filter()` and `.select()` are *lazy* commands in Spark, and we need to do something extra with them in order to get them to run as Spark jobs.**\n",
    "\n",
    "The reason `.partition()` and `.filter()` are lazy is because some operations are executed right away in Spark, and some are not\n",
    "\n",
    "\n",
    "# Actions vs. Transformations\n",
    "- **Actions** are code that is executed *immediately* (eager)\n",
    "    - Examples include `.show()`, `.take()`, `.head()`, `.write_csv()`, `.write_parquet()` etc.\n",
    "- **Transformations** are code that is *lazy* (i.e., *NOT* executed immediately)\n",
    "    - Examples include selecting columns, data filtering, JOIN's, GROUP BY operations, etc.\n",
    "    - In these cases, Spark creates a sequence of transformations that is executed *only* when we call some method like `.show()`, which is an example of an Action\n",
    "\n",
    "***To summarize, Spark creates a sequence of Transformations that aren't executed until an Action is executed***\n",
    "\n",
    "**See more at**:\n",
    "- https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "- https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/\n",
    "- https://data-flair.training/blogs/apache-spark-lazy-evaluation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a335258d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2021, 1, 3, 15, 59, 58), dropoff_datetime=datetime.datetime(2021, 1, 3, 16, 13, 50), PULocationID=144, DOLocationID=261),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 1, 14, 39, 29), dropoff_datetime=datetime.datetime(2021, 1, 1, 14, 59, 45), PULocationID=148, DOLocationID=68),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 1, 7, 25, 16), dropoff_datetime=datetime.datetime(2021, 1, 1, 7, 50, 46), PULocationID=61, DOLocationID=76),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 2, 1, 5, 28), dropoff_datetime=datetime.datetime(2021, 1, 2, 1, 11, 40), PULocationID=42, DOLocationID=42),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 2, 13, 1, 44), dropoff_datetime=datetime.datetime(2021, 1, 2, 13, 25, 23), PULocationID=155, DOLocationID=177)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do filtering and execute a job (i.e. a filter transformation) by performing a Spark Action\n",
    "df_spark.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "    .filter(df_spark.hvfhs_license_num == 'HV0003') \\\n",
    "    .take(5)  # or .head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14c149f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0004', count=110015),\n",
       " Row(hvfhs_license_num='HV0005', count=3094325),\n",
       " Row(hvfhs_license_num='HV0003', count=8704128)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do a Group by's with a Collect action\n",
    "df_spark.groupBy(df_spark.hvfhs_license_num) \\\n",
    "    .count() \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c50c2",
   "metadata": {},
   "source": [
    "# Spark Functions\n",
    "\n",
    "#### Why bother with the above filter statement in Spark when `SELECT * FROM df WHERE hvfhs_license_num = 'HV0003'` in SQL works just fine?\n",
    "\n",
    "We do because **Spark is more flexible**, and gives us the ability to create **user-defined functions (UDFs)**\n",
    "\n",
    "But, before we get into UDFs, we first look at **Spark-provided functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c0e5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a collection of functions that Spark already has\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dab0d9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Any',\n",
       " 'ArrayType',\n",
       " 'Callable',\n",
       " 'Column',\n",
       " 'DataFrame',\n",
       " 'DataType',\n",
       " 'Dict',\n",
       " 'Iterable',\n",
       " 'List',\n",
       " 'Optional',\n",
       " 'PandasUDFType',\n",
       " 'PythonEvalType',\n",
       " 'SparkContext',\n",
       " 'StringType',\n",
       " 'StructType',\n",
       " 'TYPE_CHECKING',\n",
       " 'Tuple',\n",
       " 'Union',\n",
       " 'UserDefinedFunction',\n",
       " 'ValuesView',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_create_column_from_literal',\n",
       " '_create_lambda',\n",
       " '_create_udf',\n",
       " '_get_jvm_function',\n",
       " '_get_lambda_parameters',\n",
       " '_invoke_binary_math_function',\n",
       " '_invoke_function',\n",
       " '_invoke_function_over_columns',\n",
       " '_invoke_function_over_seq_of_columns',\n",
       " '_invoke_higher_order_function',\n",
       " '_options_to_str',\n",
       " '_test',\n",
       " '_to_java_column',\n",
       " '_to_seq',\n",
       " '_unresolved_named_lambda_variable',\n",
       " 'abs',\n",
       " 'acos',\n",
       " 'acosh',\n",
       " 'add_months',\n",
       " 'aggregate',\n",
       " 'approxCountDistinct',\n",
       " 'approx_count_distinct',\n",
       " 'array',\n",
       " 'array_contains',\n",
       " 'array_distinct',\n",
       " 'array_except',\n",
       " 'array_intersect',\n",
       " 'array_join',\n",
       " 'array_max',\n",
       " 'array_min',\n",
       " 'array_position',\n",
       " 'array_remove',\n",
       " 'array_repeat',\n",
       " 'array_sort',\n",
       " 'array_union',\n",
       " 'arrays_overlap',\n",
       " 'arrays_zip',\n",
       " 'asc',\n",
       " 'asc_nulls_first',\n",
       " 'asc_nulls_last',\n",
       " 'ascii',\n",
       " 'asin',\n",
       " 'asinh',\n",
       " 'assert_true',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atanh',\n",
       " 'avg',\n",
       " 'base64',\n",
       " 'bin',\n",
       " 'bit_length',\n",
       " 'bitwiseNOT',\n",
       " 'bitwise_not',\n",
       " 'broadcast',\n",
       " 'bround',\n",
       " 'bucket',\n",
       " 'cast',\n",
       " 'cbrt',\n",
       " 'ceil',\n",
       " 'coalesce',\n",
       " 'col',\n",
       " 'collect_list',\n",
       " 'collect_set',\n",
       " 'column',\n",
       " 'concat',\n",
       " 'concat_ws',\n",
       " 'conv',\n",
       " 'corr',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'cot',\n",
       " 'count',\n",
       " 'countDistinct',\n",
       " 'count_distinct',\n",
       " 'covar_pop',\n",
       " 'covar_samp',\n",
       " 'crc32',\n",
       " 'create_map',\n",
       " 'csc',\n",
       " 'cume_dist',\n",
       " 'current_date',\n",
       " 'current_timestamp',\n",
       " 'date_add',\n",
       " 'date_format',\n",
       " 'date_sub',\n",
       " 'date_trunc',\n",
       " 'datediff',\n",
       " 'dayofmonth',\n",
       " 'dayofweek',\n",
       " 'dayofyear',\n",
       " 'days',\n",
       " 'decode',\n",
       " 'degrees',\n",
       " 'dense_rank',\n",
       " 'desc',\n",
       " 'desc_nulls_first',\n",
       " 'desc_nulls_last',\n",
       " 'element_at',\n",
       " 'encode',\n",
       " 'exists',\n",
       " 'exp',\n",
       " 'explode',\n",
       " 'explode_outer',\n",
       " 'expm1',\n",
       " 'expr',\n",
       " 'factorial',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'flatten',\n",
       " 'floor',\n",
       " 'forall',\n",
       " 'format_number',\n",
       " 'format_string',\n",
       " 'from_csv',\n",
       " 'from_json',\n",
       " 'from_unixtime',\n",
       " 'from_utc_timestamp',\n",
       " 'functools',\n",
       " 'get_json_object',\n",
       " 'greatest',\n",
       " 'grouping',\n",
       " 'grouping_id',\n",
       " 'hash',\n",
       " 'hex',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'hypot',\n",
       " 'initcap',\n",
       " 'input_file_name',\n",
       " 'inspect',\n",
       " 'instr',\n",
       " 'isnan',\n",
       " 'isnull',\n",
       " 'json_tuple',\n",
       " 'kurtosis',\n",
       " 'lag',\n",
       " 'last',\n",
       " 'last_day',\n",
       " 'lead',\n",
       " 'least',\n",
       " 'length',\n",
       " 'levenshtein',\n",
       " 'lit',\n",
       " 'locate',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'lower',\n",
       " 'lpad',\n",
       " 'ltrim',\n",
       " 'make_date',\n",
       " 'map_concat',\n",
       " 'map_entries',\n",
       " 'map_filter',\n",
       " 'map_from_arrays',\n",
       " 'map_from_entries',\n",
       " 'map_keys',\n",
       " 'map_values',\n",
       " 'map_zip_with',\n",
       " 'max',\n",
       " 'max_by',\n",
       " 'md5',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'min_by',\n",
       " 'minute',\n",
       " 'monotonically_increasing_id',\n",
       " 'month',\n",
       " 'months',\n",
       " 'months_between',\n",
       " 'nanvl',\n",
       " 'next_day',\n",
       " 'nth_value',\n",
       " 'ntile',\n",
       " 'octet_length',\n",
       " 'overlay',\n",
       " 'overload',\n",
       " 'pandas_udf',\n",
       " 'percent_rank',\n",
       " 'percentile_approx',\n",
       " 'posexplode',\n",
       " 'posexplode_outer',\n",
       " 'pow',\n",
       " 'product',\n",
       " 'quarter',\n",
       " 'radians',\n",
       " 'raise_error',\n",
       " 'rand',\n",
       " 'randn',\n",
       " 'rank',\n",
       " 'regexp_extract',\n",
       " 'regexp_replace',\n",
       " 'repeat',\n",
       " 'reverse',\n",
       " 'rint',\n",
       " 'round',\n",
       " 'row_number',\n",
       " 'rpad',\n",
       " 'rtrim',\n",
       " 'schema_of_csv',\n",
       " 'schema_of_json',\n",
       " 'sec',\n",
       " 'second',\n",
       " 'sentences',\n",
       " 'sequence',\n",
       " 'session_window',\n",
       " 'sha1',\n",
       " 'sha2',\n",
       " 'shiftLeft',\n",
       " 'shiftRight',\n",
       " 'shiftRightUnsigned',\n",
       " 'shiftleft',\n",
       " 'shiftright',\n",
       " 'shiftrightunsigned',\n",
       " 'shuffle',\n",
       " 'signum',\n",
       " 'sin',\n",
       " 'since',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'skewness',\n",
       " 'slice',\n",
       " 'sort_array',\n",
       " 'soundex',\n",
       " 'spark_partition_id',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'stddev',\n",
       " 'stddev_pop',\n",
       " 'stddev_samp',\n",
       " 'struct',\n",
       " 'substring',\n",
       " 'substring_index',\n",
       " 'sum',\n",
       " 'sumDistinct',\n",
       " 'sum_distinct',\n",
       " 'sys',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'timestamp_seconds',\n",
       " 'toDegrees',\n",
       " 'toRadians',\n",
       " 'to_csv',\n",
       " 'to_date',\n",
       " 'to_json',\n",
       " 'to_str',\n",
       " 'to_timestamp',\n",
       " 'to_utc_timestamp',\n",
       " 'transform',\n",
       " 'transform_keys',\n",
       " 'transform_values',\n",
       " 'translate',\n",
       " 'trim',\n",
       " 'trunc',\n",
       " 'udf',\n",
       " 'unbase64',\n",
       " 'unhex',\n",
       " 'unix_timestamp',\n",
       " 'upper',\n",
       " 'var_pop',\n",
       " 'var_samp',\n",
       " 'variance',\n",
       " 'warnings',\n",
       " 'weekofyear',\n",
       " 'when',\n",
       " 'window',\n",
       " 'xxhash64',\n",
       " 'year',\n",
       " 'years',\n",
       " 'zip_with']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # type in `F.` and hit TAB to see the list of functions\n",
    "# F.\n",
    "\n",
    "# Or do this\n",
    "dir(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da417c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|pickup_date|dropoff_date|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+\n",
      "|           HV0005|              B02510|2021-01-02 11:31:29|2021-01-02 11:37:35|          28|         130|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0003|              B02877|2021-01-03 15:59:58|2021-01-03 16:13:50|         144|         261|   null| 2021-01-03|  2021-01-03|\n",
      "|           HV0005|              B02510|2021-01-02 20:41:20|2021-01-02 20:58:35|         138|         232|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0005|              B02510|2021-01-02 12:32:53|2021-01-02 12:37:51|          42|         116|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0003|              B02869|2021-01-01 14:39:29|2021-01-01 14:59:45|         148|          68|   null| 2021-01-01|  2021-01-01|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # take a datetime and keep only the date\n",
    "# F.to_date()\n",
    "\n",
    "# Add a new column to the DataFrame wherein we take a datetime column and keep ONLY the DATE\n",
    "df_spark \\\n",
    "    .withColumn('pickup_date', F.to_date(df_spark.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df_spark.dropoff_datetime)) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "774618a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-02|  2021-01-02|          28|         130|\n",
      "| 2021-01-03|  2021-01-03|         144|         261|\n",
      "| 2021-01-02|  2021-01-02|         138|         232|\n",
      "| 2021-01-02|  2021-01-02|          42|         116|\n",
      "| 2021-01-01|  2021-01-01|         148|          68|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the same as above but select only a few columns\n",
    "df_spark \\\n",
    "    .withColumn('pickup_date', F.to_date(df_spark.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df_spark.dropoff_datetime)) \\\n",
    "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f18bf1",
   "metadata": {},
   "source": [
    "# User-Defined Functions\n",
    "\n",
    "**Again, we can also define our *own* functions**\n",
    "\n",
    "**This is not something we'd typically do in a data warehouse because it can become cumbersome**\n",
    "\n",
    "**But in PySpark, we can store all the code easily in a repository, cover it with *tests*, and really make sure the code works before executing it on DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3e32d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a UDF\n",
    "def cant_do_in_sql(base_num):\n",
    "    \n",
    "    num = int(base_num[1:])\n",
    "    \n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7f2e9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s/b44'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the UDF\n",
    "cant_do_in_sql('B02884')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363eb79d",
   "metadata": {},
   "source": [
    "#### The function above can live in a separate Python module, and we can test it with *unit tests*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f457fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "# Turn our user-defined Python function into a *Spark* function using Spark's Functions library (F)\n",
    "cant_do_in_sql_udf = F.udf(cant_do_in_sql, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77f8acb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  e/9ce| 2021-01-02|  2021-01-02|          28|         130|\n",
      "|  s/b3d| 2021-01-03|  2021-01-03|         144|         261|\n",
      "|  e/9ce| 2021-01-02|  2021-01-02|         138|         232|\n",
      "|  e/9ce| 2021-01-02|  2021-01-02|          42|         116|\n",
      "|  e/b35| 2021-01-01|  2021-01-01|         148|          68|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the Spark UDF\n",
    "df_spark \\\n",
    "    .withColumn('pickup_date', F.to_date(df_spark.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df_spark.dropoff_datetime)) \\\n",
    "    .withColumn('base_id', cant_do_in_sql_udf(df_spark.dispatching_base_num)) \\\n",
    "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-zoom]",
   "language": "python",
   "name": "conda-env-.conda-zoom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
