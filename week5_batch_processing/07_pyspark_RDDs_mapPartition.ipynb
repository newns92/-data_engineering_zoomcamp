{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e89862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# instantiate a Spark session, an object that we use to interact with Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9675d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----------------+-----+\n",
      "|               hour|zone|          revenue|count|\n",
      "+-------------------+----+-----------------+-----+\n",
      "|2020-01-18 11:00:00| 128|            33.51|    1|\n",
      "|2020-01-08 20:00:00| 197|             10.3|    1|\n",
      "|2020-01-02 22:00:00|  82|261.4800000000001|   22|\n",
      "|2020-01-21 18:00:00| 220|           118.56|    2|\n",
      "|2020-01-22 11:00:00| 257|           141.82|    3|\n",
      "+-------------------+----+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get our data\n",
    "df_result = spark.read.parquet('tmp/green-revenue')\n",
    "\n",
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76c39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in green data\n",
    "df_green = spark.read.parquet('data/parquet/green/*/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8194ea8",
   "metadata": {},
   "source": [
    "**`.mapPartitions()` function returns a *new* RDD by applying a function to *each partition* of a provided RDD**\n",
    "- `.mapPartitions()` receives a data partition (via an RDD) as input, applies some function to that data, and produces *another* partition as output (via an RDD)\n",
    "     - `.mapPartitions()` is *exactly* the same as `.map()`, with the difference being that `.mapPartitions()` provides a facility to do heavy initializations (for example, Database connections) *once for each partition* instead of doing it on *every DataFrame row*\n",
    "         - This helps the performance of a job when dealing with heavy-weighted initialization on larger datasets\n",
    "     - See \"Spark `.map()` vs `.mapPartitions()` with Examples\" for more: https://sparkbyexamples.com/spark/spark-map-vs-mappartitions-transformation/\n",
    "- Suppose we have a 1TB dataset partitioned into chunks of 100MB\n",
    "    - We want to process each chunk and produce *another* partition with *processed* data\n",
    "    - Many applications benefit from this sequence of steps, such as ML\n",
    "    - Suppose we have a trained ML model that we put *inside* the .`mapPartition()` function\n",
    "    - Then, Spark will **chunk** a large dataset into smaller partitions and apply our model *to each partition*, outputting its predictions on said partition\n",
    "\n",
    "**As a first example/use case, we implement a simple method to count the number of rows in each partition, to illustrate how to use `.mapPartitions()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac357b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows(partition):\n",
    "    count = 0\n",
    "    for row in partition:\n",
    "        count += 1\n",
    "    return [count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40841825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[447770,\n",
       " 454484,\n",
       " 238894,\n",
       " 154607,\n",
       " 154692,\n",
       " 149681,\n",
       " 149463,\n",
       " 144966,\n",
       " 135114,\n",
       " 117302,\n",
       " 107592,\n",
       " 49952]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['VendorID', 'lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance']\n",
    "\n",
    "duration_rdd = df_green \\\n",
    "    .select(columns) \\\n",
    "    .rdd\n",
    "\n",
    "duration_rdd.mapPartitions(count_rows).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197fbaec",
   "metadata": {},
   "source": [
    "**We can also reimplement `count_rows()` using `pandas`**\n",
    "- Note that although a `pandas.DataFrame` is simpler to manipulate, this approach *materializes the entire DataFrame in memory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f0e2cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[447770,\n",
       " 454484,\n",
       " 238894,\n",
       " 154607,\n",
       " 154692,\n",
       " 149681,\n",
       " 149463,\n",
       " 144966,\n",
       " 135114,\n",
       " 117302,\n",
       " 107592,\n",
       " 49952]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_rows_using_pandas(partition):\n",
    "    df = pd.DataFrame(partition, columns=columns)\n",
    "    \n",
    "    return [len(df)]\n",
    "\n",
    "duration_rdd.mapPartitions(count_rows_using_pandas).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016eefdd",
   "metadata": {},
   "source": [
    "**See that we get the same results**\n",
    "\n",
    "**Now, let's suppose we have a simple ML model that predicts trip duration as trip distance multiplied by 5 (i.e., we will create an application that predict the duration of a trips)**\n",
    "\n",
    "**First, select the necessary columns and turn this to a RDD so we can apply our upcoming model to each partition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2642a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 22, 13, 18, 32), PULocationID=244, DOLocationID=41, trip_distance=5.22),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 17, 54, 10), PULocationID=236, DOLocationID=263, trip_distance=0.87),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 19, 10, 23, 37), PULocationID=166, DOLocationID=166, trip_distance=0.63)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the necessary columns and turn the DataFrame to a RDD\n",
    "columns = ['VendorID', 'lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance']\n",
    "\n",
    "duration_rdd = df_green \\\n",
    "    .select(columns) \\\n",
    "    .rdd\n",
    "\n",
    "duration_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "850486e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The code below returns [1] for each partitions and flattens the list\n",
    "# def apply_model_in_batch(partition):\n",
    "#     return [1]\n",
    "\n",
    "# duration_rdd.mapPartitions(apply_model_in_batch).collect()\n",
    "# # [1, 1, 1, 1]  # four partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39a21f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[447770,\n",
       " 454484,\n",
       " 238894,\n",
       " 154607,\n",
       " 154692,\n",
       " 149681,\n",
       " 149463,\n",
       " 144966,\n",
       " 135114,\n",
       " 117302,\n",
       " 107592,\n",
       " 49952]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code below returns the number of rows per for each partition and flattens the list\n",
    "def apply_model_in_batch(partition):\n",
    "    cnt = 0\n",
    "\n",
    "    for row in partition:\n",
    "        cnt = cnt + 1\n",
    "\n",
    "    return [cnt]\n",
    "\n",
    "duration_rdd.mapPartitions(apply_model_in_batch).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f033e6",
   "metadata": {},
   "source": [
    "**We see that our partitions are not really balanced. 2 (maybe 3) partitions are very large compared to others**\n",
    "\n",
    "**Let's turn partitions to a `pandas` DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "996dbbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-22 13:18:32</td>\n",
       "      <td>244</td>\n",
       "      <td>41</td>\n",
       "      <td>5.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-23 17:54:10</td>\n",
       "      <td>236</td>\n",
       "      <td>263</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-19 10:23:37</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-21 14:25:16</td>\n",
       "      <td>152</td>\n",
       "      <td>238</td>\n",
       "      <td>2.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-07 09:46:00</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>2.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-10 08:19:00</td>\n",
       "      <td>32</td>\n",
       "      <td>186</td>\n",
       "      <td>17.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-15 10:57:40</td>\n",
       "      <td>130</td>\n",
       "      <td>228</td>\n",
       "      <td>24.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-26 16:52:02</td>\n",
       "      <td>7</td>\n",
       "      <td>223</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-24 21:57:19</td>\n",
       "      <td>244</td>\n",
       "      <td>87</td>\n",
       "      <td>10.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-27 23:45:54</td>\n",
       "      <td>65</td>\n",
       "      <td>62</td>\n",
       "      <td>3.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID lpep_pickup_datetime  PULocationID  DOLocationID  trip_distance\n",
       "0       2.0  2020-01-22 13:18:32           244            41           5.22\n",
       "1       2.0  2020-01-23 17:54:10           236           263           0.87\n",
       "2       2.0  2020-01-19 10:23:37           166           166           0.63\n",
       "3       2.0  2020-01-21 14:25:16           152           238           2.71\n",
       "4       NaN  2020-01-07 09:46:00            51             3           2.13\n",
       "5       NaN  2020-01-10 08:19:00            32           186          17.93\n",
       "6       2.0  2020-01-15 10:57:40           130           228          24.04\n",
       "7       2.0  2020-01-26 16:52:02             7           223           1.45\n",
       "8       2.0  2020-01-24 21:57:19           244            87          10.94\n",
       "9       2.0  2020-01-27 23:45:54            65            62           3.18"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = duration_rdd.take(10)\n",
    "\n",
    "pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ff89b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[447770,\n",
       " 454484,\n",
       " 238894,\n",
       " 154607,\n",
       " 154692,\n",
       " 149681,\n",
       " 149463,\n",
       " 144966,\n",
       " 135114,\n",
       " 117302,\n",
       " 107592,\n",
       " 49952]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_model_in_batch(rows):\n",
    "    # WARNING: the immediate below line materializes the entire DataFrame in memory\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    \n",
    "    cnt = len(df)\n",
    "    \n",
    "    return [cnt]\n",
    "\n",
    "duration_rdd.mapPartitions(apply_model_in_batch).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a1b27",
   "metadata": {},
   "source": [
    "**The above put the entire partition in a DataFrame, *which isn’t always good***\n",
    "\n",
    "**If you want to solve it, you can use `.islice()` to break a partition into 100,000-row subpartitions and treat them them separately**\n",
    "\n",
    "**Anyway, now, we are ready to apply a ML model**\n",
    "\n",
    "**Normally we'd have a model that calculates predictions from an algorithm + data from a DataFrame. But since we don’t have a model yet, let’s calculate an arbitrary duration (5 minutes per km)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e186821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(df):\n",
    "    #y_pred = model.predict(df)\n",
    "    y_pred = df.trip_distance * 5\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3dccd9",
   "metadata": {},
   "source": [
    "**We can also use `itertuples()` to iterate over DataFrame rows as namedtuples. For illustrative purposes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45f028c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pandas(Index=0, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-22 13:18:32'), PULocationID=244, DOLocationID=41, trip_distance=5.22),\n",
       " Pandas(Index=1, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-23 17:54:10'), PULocationID=236, DOLocationID=263, trip_distance=0.87),\n",
       " Pandas(Index=2, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-19 10:23:37'), PULocationID=166, DOLocationID=166, trip_distance=0.63),\n",
       " Pandas(Index=3, VendorID=2.0, lpep_pickup_datetime=Timestamp('2020-01-21 14:25:16'), PULocationID=152, DOLocationID=238, trip_distance=2.71),\n",
       " Pandas(Index=4, VendorID=nan, lpep_pickup_datetime=Timestamp('2020-01-07 09:46:00'), PULocationID=51, DOLocationID=3, trip_distance=2.13)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = duration_rdd.take(5)\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "list(df.itertuples())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8377dddc",
   "metadata": {},
   "source": [
    "**The `yield` keyword in Python is similar to a `return` statement used for returning values or objects in Python**\n",
    "\n",
    "**However, there is a *slight* difference: The `yield` statement returns a `generator` object to the one who calls the function which contains `yield`, instead of simply returning a value. For example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e012b8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object infinite_seq at 0x000002CA0CF69BA0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def infinite_seq(flag: bool):\n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        yield i\n",
    "        i = i + 1\n",
    "\n",
    "        if flag and i > 10:\n",
    "            break\n",
    "            \n",
    "# produce an infinite sequence            \n",
    "seq = infinite_seq(False)\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15c5dcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# produces a finite sequence\n",
    "seq = infinite_seq(True)\n",
    "# seq\n",
    "list(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc4bd4f",
   "metadata": {},
   "source": [
    "**So, `yield` writes each `Row` to the resulting RDD and then it will flatten it**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7705689",
   "metadata": {},
   "source": [
    "**We can use these (`yield` and `.itertuples()` in our ML model batch processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a04acd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_in_batch(rows):\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    \n",
    "    # do our \"prediction\"\n",
    "    predictions = model_predict(df)\n",
    "    \n",
    "    df['predicted_duration'] = predictions\n",
    "    \n",
    "    # iterate over the DataFrame rows as tuples\n",
    "    for row in df.itertuples():\n",
    "        # yield (return) the row\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1096e969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the predictions\n",
    "df_predictions = duration_rdd \\\n",
    "    .mapPartitions(apply_model_in_batch) \\\n",
    "    .toDF() \\\n",
    "    .drop('Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1984e0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|predicted_duration|\n",
      "+------------------+\n",
      "|26.099999999999998|\n",
      "|              4.35|\n",
      "|              3.15|\n",
      "|             13.55|\n",
      "|10.649999999999999|\n",
      "|             89.65|\n",
      "|120.19999999999999|\n",
      "|              7.25|\n",
      "|54.699999999999996|\n",
      "|              15.9|\n",
      "|               9.3|\n",
      "|              8.75|\n",
      "|             94.75|\n",
      "|              46.2|\n",
      "|             16.85|\n",
      "|              14.6|\n",
      "|             25.65|\n",
      "|               5.0|\n",
      "|2.9499999999999997|\n",
      "|               0.0|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect the predictions\n",
    "df_predictions.select('predicted_duration').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc6c8f",
   "metadata": {},
   "source": [
    "**In the real world, this type of application would use real-time (*streaming*) processing rather than *batch* processing, since we'd like to inform the estimated time that the trip will take to the user as soon as they request a ride in their app**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-zoom]",
   "language": "python",
   "name": "conda-env-.conda-zoom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
