{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23146a50",
   "metadata": {},
   "source": [
    "# Make sure you run `pyspark_intro.ipynb` to write the partitioned data to the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9b60ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# instantiate a Spark session, an object that we use to interact with Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d55ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009456a1",
   "metadata": {},
   "source": [
    "**You will be able to see all 24 partitions as files in the `fhvhv/2021/01/`dir**\n",
    "\n",
    "**Can also see a `parquet` job at http://localhost:4040/jobs/ that we can click into and view more information such as DAGs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eec55fb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 08A3-CF2D\n",
      "\n",
      " Directory of C:\\Users\\nimz\\Documents\\de_zoomcamp\\week5_batch_processing\\fhvhv\\2021\\01\n",
      "\n",
      "05/09/2023  08:47 PM    <DIR>          .\n",
      "05/09/2023  08:47 PM    <DIR>          ..\n",
      "05/09/2023  08:47 PM            71,484 .part-00000-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,468 .part-00001-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,476 .part-00002-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,480 .part-00003-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,480 .part-00004-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,484 .part-00005-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,444 .part-00006-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,480 .part-00007-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,464 .part-00008-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,468 .part-00009-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,456 .part-00010-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,464 .part-00011-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,500 .part-00012-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,476 .part-00013-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,512 .part-00014-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,468 .part-00015-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,524 .part-00016-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,488 .part-00017-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,508 .part-00018-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,472 .part-00019-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,492 .part-00020-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,444 .part-00021-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,460 .part-00022-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,496 .part-00023-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM                 8 ._SUCCESS.crc\n",
      "05/09/2023  08:47 PM         9,148,662 part-00000-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,467 part-00001-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,147,559 part-00002-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,148,033 part-00003-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,148,117 part-00004-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,148,532 part-00005-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,143,335 part-00006-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,148,092 part-00007-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,261 part-00008-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,732 part-00009-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,145,061 part-00010-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,075 part-00011-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,150,733 part-00012-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,147,636 part-00013-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,152,105 part-00014-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,498 part-00015-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,153,804 part-00016-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,149,358 part-00017-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,151,965 part-00018-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,147,383 part-00019-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,149,612 part-00020-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,143,484 part-00021-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,145,646 part-00022-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,150,453 part-00023-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM                 0 _SUCCESS\n",
      "              50 File(s)    221,267,099 bytes\n",
      "               2 Dir(s)  368,540,135,424 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir fhvhv\\2021\\01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98557b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the partitioned files back into a Spark dataframe\n",
    "df_spark = spark.read.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f871967a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at the DataFrame schema \n",
    "#    - parquet files are smaller because they know the schema and use more efficient ways of compressing data\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7731b6d4",
   "metadata": {},
   "source": [
    "## What can we do with Spark DataFrames?\n",
    "\n",
    "We can do the usual stuff we do with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd1a082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select only specific columns\n",
    "df_spark.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d384ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-03 15:59:58|2021-01-03 16:13:50|         144|         261|\n",
      "|2021-01-01 14:39:29|2021-01-01 14:59:45|         148|          68|\n",
      "|2021-01-01 07:25:16|2021-01-01 07:50:46|          61|          76|\n",
      "|2021-01-02 01:05:28|2021-01-02 01:11:40|          42|          42|\n",
      "|2021-01-02 13:01:44|2021-01-02 13:25:23|         155|         177|\n",
      "|2021-01-01 05:51:46|2021-01-01 06:03:24|          49|         177|\n",
      "|2021-01-01 02:12:08|2021-01-01 02:19:49|          94|         174|\n",
      "|2021-01-01 02:17:17|2021-01-01 02:34:03|          42|           4|\n",
      "|2021-01-01 01:05:04|2021-01-01 01:17:42|         231|         265|\n",
      "|2021-01-03 01:05:38|2021-01-03 01:09:14|         229|         141|\n",
      "|2021-01-03 00:37:31|2021-01-03 01:01:18|         179|          14|\n",
      "|2021-01-01 17:23:04|2021-01-01 17:44:37|          76|          91|\n",
      "|2021-01-01 21:10:25|2021-01-01 21:30:13|         263|          69|\n",
      "|2021-01-01 14:22:54|2021-01-01 14:46:51|         229|          73|\n",
      "|2021-01-01 15:50:41|2021-01-01 15:56:03|         130|         130|\n",
      "|2021-01-01 20:19:48|2021-01-01 20:31:48|         241|         254|\n",
      "|2021-01-01 21:56:48|2021-01-01 22:04:26|         144|         231|\n",
      "|2021-01-01 19:54:28|2021-01-01 19:59:41|         159|          69|\n",
      "|2021-01-03 10:14:11|2021-01-03 10:23:06|          29|          21|\n",
      "|2021-01-02 17:02:12|2021-01-02 17:22:48|          88|         265|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do filtering\n",
    "df_spark.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "    .filter(df_spark.hvfhs_license_num == 'HV0003') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9229c",
   "metadata": {},
   "source": [
    "The reason `.partition()` and `.filter()` are lazy is because some operations are executed right away in Spark, and some are not\n",
    "\n",
    "# Actions vs. Transformations\n",
    "- **Actions** = code that is executed immediately (eager)\n",
    "    - `show()`, `take()`, `head()`, `write()`, etc.\n",
    "- **Transformations** = code that is lazy (i.e., not executed immediately)\n",
    "    - Selecting columns, data filtering, JOIN's , and GROUP BY operations\n",
    "    - In these cases, Spark creates a sequence of transformations that is executed only when we call some method like `show()`, which is an example of an Action.\n",
    "\n",
    "***Spark creates a sequence of transformations until an action is executed***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b64f006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2021, 1, 3, 15, 59, 58), dropoff_datetime=datetime.datetime(2021, 1, 3, 16, 13, 50), PULocationID=144, DOLocationID=261),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 1, 14, 39, 29), dropoff_datetime=datetime.datetime(2021, 1, 1, 14, 59, 45), PULocationID=148, DOLocationID=68),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 1, 7, 25, 16), dropoff_datetime=datetime.datetime(2021, 1, 1, 7, 50, 46), PULocationID=61, DOLocationID=76),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 2, 1, 5, 28), dropoff_datetime=datetime.datetime(2021, 1, 2, 1, 11, 40), PULocationID=42, DOLocationID=42),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 2, 13, 1, 44), dropoff_datetime=datetime.datetime(2021, 1, 2, 13, 25, 23), PULocationID=155, DOLocationID=177)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do filtering\n",
    "df_spark.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "    .filter(df_spark.hvfhs_license_num == 'HV0003') \\\n",
    "    .take(5)  # or .head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa9f9c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do group by's\n",
    "# df_spark.groupBy() \\\n",
    "#     .select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "#     .filter(df_spark.hvfhs_license_num == 'HV0003') \\\n",
    "#     .take(5)  # or .head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419dd3c",
   "metadata": {},
   "source": [
    "Why bother with the above when `SELECT * FROM df WHERE hvfhs_license_num = 'HV0003'` in SQL works?\n",
    "\n",
    "Spark is more flexible, and gives us **user-defined functions**\n",
    "\n",
    "But before we get into that, we can look at **Spark-provided functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e4f0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection of functions Spark already has\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1462fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # type in \"F.\" and hit TAB to see the list of functions\n",
    "# F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52ab256b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|pickup_date|dropoff_date|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+\n",
      "|           HV0005|              B02510|2021-01-02 11:31:29|2021-01-02 11:37:35|          28|         130|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0003|              B02877|2021-01-03 15:59:58|2021-01-03 16:13:50|         144|         261|   null| 2021-01-03|  2021-01-03|\n",
      "|           HV0005|              B02510|2021-01-02 20:41:20|2021-01-02 20:58:35|         138|         232|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0005|              B02510|2021-01-02 12:32:53|2021-01-02 12:37:51|          42|         116|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0003|              B02869|2021-01-01 14:39:29|2021-01-01 14:59:45|         148|          68|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0003|              B02880|2021-01-01 07:25:16|2021-01-01 07:50:46|          61|          76|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0003|              B02878|2021-01-02 01:05:28|2021-01-02 01:11:40|          42|          42|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0003|              B02887|2021-01-02 13:01:44|2021-01-02 13:25:23|         155|         177|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0003|              B02878|2021-01-01 05:51:46|2021-01-01 06:03:24|          49|         177|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0003|              B02765|2021-01-01 02:12:08|2021-01-01 02:19:49|          94|         174|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0003|              B02879|2021-01-01 02:17:17|2021-01-01 02:34:03|          42|           4|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0003|              B02764|2021-01-01 01:05:04|2021-01-01 01:17:42|         231|         265|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0003|              B02883|2021-01-03 01:05:38|2021-01-03 01:09:14|         229|         141|   null| 2021-01-03|  2021-01-03|\n",
      "|           HV0003|              B02880|2021-01-03 00:37:31|2021-01-03 01:01:18|         179|          14|   null| 2021-01-03|  2021-01-03|\n",
      "|           HV0003|              B02867|2021-01-01 17:23:04|2021-01-01 17:44:37|          76|          91|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0005|              B02510|2021-01-03 18:43:20|2021-01-03 18:54:21|           7|         129|   null| 2021-01-03|  2021-01-03|\n",
      "|           HV0003|              B02878|2021-01-01 21:10:25|2021-01-01 21:30:13|         263|          69|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0005|              B02510|2021-01-02 03:18:36|2021-01-02 03:42:58|         262|         198|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0003|              B02764|2021-01-01 14:22:54|2021-01-01 14:46:51|         229|          73|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0003|              B02395|2021-01-01 15:50:41|2021-01-01 15:56:03|         130|         130|   null| 2021-01-01|  2021-01-01|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # take a datetime and keep only the date\n",
    "# F.to_date()\n",
    "\n",
    "# add a new column to the dataframe\n",
    "df_spark \\\n",
    "    .withColumn('pickup_date', F.to_date(df_spark.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df_spark.dropoff_datetime)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0cdb944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-02|  2021-01-02|          28|         130|\n",
      "| 2021-01-03|  2021-01-03|         144|         261|\n",
      "| 2021-01-02|  2021-01-02|         138|         232|\n",
      "| 2021-01-02|  2021-01-02|          42|         116|\n",
      "| 2021-01-01|  2021-01-01|         148|          68|\n",
      "| 2021-01-01|  2021-01-01|          61|          76|\n",
      "| 2021-01-02|  2021-01-02|          42|          42|\n",
      "| 2021-01-02|  2021-01-02|         155|         177|\n",
      "| 2021-01-01|  2021-01-01|          49|         177|\n",
      "| 2021-01-01|  2021-01-01|          94|         174|\n",
      "| 2021-01-01|  2021-01-01|          42|           4|\n",
      "| 2021-01-01|  2021-01-01|         231|         265|\n",
      "| 2021-01-03|  2021-01-03|         229|         141|\n",
      "| 2021-01-03|  2021-01-03|         179|          14|\n",
      "| 2021-01-01|  2021-01-01|          76|          91|\n",
      "| 2021-01-03|  2021-01-03|           7|         129|\n",
      "| 2021-01-01|  2021-01-01|         263|          69|\n",
      "| 2021-01-02|  2021-01-02|         262|         198|\n",
      "| 2021-01-01|  2021-01-01|         229|          73|\n",
      "| 2021-01-01|  2021-01-01|         130|         130|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # take a datetime and keep only the date\n",
    "# F.to_date()\n",
    "\n",
    "# add a new column to the dataframe\n",
    "df_spark \\\n",
    "    .withColumn('pickup_date', F.to_date(df_spark.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df_spark.dropoff_datetime)) \\\n",
    "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b4b21",
   "metadata": {},
   "source": [
    "**Again, we can also define our own functions**\n",
    "\n",
    "**This is not something we'd typically do in data warehouses, because it can be cumbersome**\n",
    "\n",
    "**But in PySpark, we can store all the code easily, cover it with tests, and really make sure the code works before executing it on you dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "253984bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cant_do_in_sql(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80f45162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s/b44'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cant_do_in_sql('B02884')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e755a",
   "metadata": {},
   "source": [
    "**The above can live in a separate Python module, and we can test it with unit tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd193d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67ecbfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our user-defined Python function into a Spark function\n",
    "cant_do_in_sql_udf = F.udf(cant_do_in_sql, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87124c77",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Resolved attribute(s) pickup_datetime#2 missing from hvfhs_license_num#14,dispatching_base_num#15,pickup_datetime#16,dropoff_datetime#17,PULocationID#18,DOLocationID#19,SR_Flag#20 in operator !Project [hvfhs_license_num#14, dispatching_base_num#15, pickup_datetime#16, dropoff_datetime#17, PULocationID#18, DOLocationID#19, SR_Flag#20, to_date(pickup_datetime#2, None, Some(America/New_York)) AS pickup_date#244]. Attribute(s) with the same name appear in the operation: pickup_datetime. Please check if the right attribute(s) are used.;\n!Project [hvfhs_license_num#14, dispatching_base_num#15, pickup_datetime#16, dropoff_datetime#17, PULocationID#18, DOLocationID#19, SR_Flag#20, to_date(pickup_datetime#2, None, Some(America/New_York)) AS pickup_date#244]\n+- Relation [hvfhs_license_num#14,dispatching_base_num#15,pickup_datetime#16,dropoff_datetime#17,PULocationID#18,DOLocationID#19,SR_Flag#20] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_spark\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpickup_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickup_datetime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropoff_date\u001b[39m\u001b[38;5;124m'\u001b[39m, F\u001b[38;5;241m.\u001b[39mto_date(df\u001b[38;5;241m.\u001b[39mdropoff_datetime)) \\\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_id\u001b[39m\u001b[38;5;124m'\u001b[39m, cant_do_in_sql_udf(df\u001b[38;5;241m.\u001b[39mdispatching_base_num)) \\\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickup_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropoff_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPULocationID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOLocationID\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py:3036\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   3034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m   3035\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 3036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Resolved attribute(s) pickup_datetime#2 missing from hvfhs_license_num#14,dispatching_base_num#15,pickup_datetime#16,dropoff_datetime#17,PULocationID#18,DOLocationID#19,SR_Flag#20 in operator !Project [hvfhs_license_num#14, dispatching_base_num#15, pickup_datetime#16, dropoff_datetime#17, PULocationID#18, DOLocationID#19, SR_Flag#20, to_date(pickup_datetime#2, None, Some(America/New_York)) AS pickup_date#244]. Attribute(s) with the same name appear in the operation: pickup_datetime. Please check if the right attribute(s) are used.;\n!Project [hvfhs_license_num#14, dispatching_base_num#15, pickup_datetime#16, dropoff_datetime#17, PULocationID#18, DOLocationID#19, SR_Flag#20, to_date(pickup_datetime#2, None, Some(America/New_York)) AS pickup_date#244]\n+- Relation [hvfhs_license_num#14,dispatching_base_num#15,pickup_datetime#16,dropoff_datetime#17,PULocationID#18,DOLocationID#19,SR_Flag#20] parquet\n"
     ]
    }
   ],
   "source": [
    "df_spark \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('base_id', cant_do_in_sql_udf(df.dispatching_base_num)) \\\n",
    "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd2cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "  .filter(df.hvfhs_license_num == 'HV0003')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-zoom]",
   "language": "python",
   "name": "conda-env-.conda-zoom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
