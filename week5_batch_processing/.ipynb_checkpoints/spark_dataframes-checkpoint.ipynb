{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c49a09",
   "metadata": {},
   "source": [
    "# Make sure you run `pyspark_intro.ipynb` to write the partitioned data to the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9b60ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# instantiate a Spark session, an object that we use to interact with Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d55ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009456a1",
   "metadata": {},
   "source": [
    "**You will be able to see all 24 partitions as files in the `fhvhv/2021/01/`dir**\n",
    "\n",
    "**Can also see a `parquet` job at http://localhost:4040/jobs/ that we can click into and view more information such as DAGs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eec55fb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 08A3-CF2D\n",
      "\n",
      " Directory of C:\\Users\\nimz\\Documents\\de_zoomcamp\\week5_batch_processing\\fhvhv\\2021\\01\n",
      "\n",
      "05/09/2023  08:47 PM    <DIR>          .\n",
      "05/09/2023  08:47 PM    <DIR>          ..\n",
      "05/09/2023  08:47 PM            71,484 .part-00000-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,468 .part-00001-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,476 .part-00002-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,480 .part-00003-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,480 .part-00004-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,484 .part-00005-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,444 .part-00006-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,480 .part-00007-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,464 .part-00008-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,468 .part-00009-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,456 .part-00010-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,464 .part-00011-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,500 .part-00012-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,476 .part-00013-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,512 .part-00014-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,468 .part-00015-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,524 .part-00016-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,488 .part-00017-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,508 .part-00018-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,472 .part-00019-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,492 .part-00020-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,444 .part-00021-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,460 .part-00022-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,496 .part-00023-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM                 8 ._SUCCESS.crc\n",
      "05/09/2023  08:47 PM         9,148,662 part-00000-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,467 part-00001-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,147,559 part-00002-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,148,033 part-00003-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,148,117 part-00004-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,148,532 part-00005-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,143,335 part-00006-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,148,092 part-00007-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,261 part-00008-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,732 part-00009-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,145,061 part-00010-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,075 part-00011-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,150,733 part-00012-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,147,636 part-00013-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,152,105 part-00014-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,498 part-00015-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,153,804 part-00016-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,149,358 part-00017-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,151,965 part-00018-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,147,383 part-00019-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,149,612 part-00020-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,143,484 part-00021-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,145,646 part-00022-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,150,453 part-00023-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM                 0 _SUCCESS\n",
      "              50 File(s)    221,267,099 bytes\n",
      "               2 Dir(s)  368,540,135,424 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir fhvhv\\2021\\01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98557b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the partitioned files back into a Spark dataframe\n",
    "df_spark = spark.read.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f871967a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at the DataFrame schema \n",
    "#    - parquet files are smaller because they know the schema and use more efficient ways of compressing data\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d95f8",
   "metadata": {},
   "source": [
    "## What can we do with Spark DataFrames?\n",
    "\n",
    "We can do the usual stuff we do with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "630a870c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select only specific columns\n",
    "df_spark.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4b6da9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-03 15:59:58|2021-01-03 16:13:50|         144|         261|\n",
      "|2021-01-01 14:39:29|2021-01-01 14:59:45|         148|          68|\n",
      "|2021-01-01 07:25:16|2021-01-01 07:50:46|          61|          76|\n",
      "|2021-01-02 01:05:28|2021-01-02 01:11:40|          42|          42|\n",
      "|2021-01-02 13:01:44|2021-01-02 13:25:23|         155|         177|\n",
      "|2021-01-01 05:51:46|2021-01-01 06:03:24|          49|         177|\n",
      "|2021-01-01 02:12:08|2021-01-01 02:19:49|          94|         174|\n",
      "|2021-01-01 02:17:17|2021-01-01 02:34:03|          42|           4|\n",
      "|2021-01-01 01:05:04|2021-01-01 01:17:42|         231|         265|\n",
      "|2021-01-03 01:05:38|2021-01-03 01:09:14|         229|         141|\n",
      "|2021-01-03 00:37:31|2021-01-03 01:01:18|         179|          14|\n",
      "|2021-01-01 17:23:04|2021-01-01 17:44:37|          76|          91|\n",
      "|2021-01-01 21:10:25|2021-01-01 21:30:13|         263|          69|\n",
      "|2021-01-01 14:22:54|2021-01-01 14:46:51|         229|          73|\n",
      "|2021-01-01 15:50:41|2021-01-01 15:56:03|         130|         130|\n",
      "|2021-01-01 20:19:48|2021-01-01 20:31:48|         241|         254|\n",
      "|2021-01-01 21:56:48|2021-01-01 22:04:26|         144|         231|\n",
      "|2021-01-01 19:54:28|2021-01-01 19:59:41|         159|          69|\n",
      "|2021-01-03 10:14:11|2021-01-03 10:23:06|          29|          21|\n",
      "|2021-01-02 17:02:12|2021-01-02 17:22:48|          88|         265|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do filtering\n",
    "df_spark.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "    .filter(df_spark.hvfhs_license_num == 'HV0003') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c91c70",
   "metadata": {},
   "source": [
    "The reason `.partition()` and `.filter()` are lazy is because some operations are executed right away in Spark, and some are not\n",
    "\n",
    "# Actions vs. Transformations\n",
    "- **Actions** = code that is executed immediately (eager)\n",
    "    - `show()`, `take()`, `head()`, `write()`, etc.\n",
    "- **Transformations** = code that is lazy (i.e., not executed immediately)\n",
    "    - Selecting columns, data filtering, JOIN's , and GROUP BY operations\n",
    "    - In these cases, Spark creates a sequence of transformations that is executed only when we call some method like `show()`, which is an example of an Action.\n",
    "\n",
    "***Spark creates a sequence of transformations until an action is executed***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5e6fb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2021, 1, 3, 15, 59, 58), dropoff_datetime=datetime.datetime(2021, 1, 3, 16, 13, 50), PULocationID=144, DOLocationID=261),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 1, 14, 39, 29), dropoff_datetime=datetime.datetime(2021, 1, 1, 14, 59, 45), PULocationID=148, DOLocationID=68),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 1, 7, 25, 16), dropoff_datetime=datetime.datetime(2021, 1, 1, 7, 50, 46), PULocationID=61, DOLocationID=76),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 2, 1, 5, 28), dropoff_datetime=datetime.datetime(2021, 1, 2, 1, 11, 40), PULocationID=42, DOLocationID=42),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 2, 13, 1, 44), dropoff_datetime=datetime.datetime(2021, 1, 2, 13, 25, 23), PULocationID=155, DOLocationID=177)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do filtering\n",
    "df_spark.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "    .filter(df_spark.hvfhs_license_num == 'HV0003') \\\n",
    "    .take(5)  # or .head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0f39494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do group by's\n",
    "# df_spark.groupBy() \\\n",
    "#     .select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "#     .filter(df_spark.hvfhs_license_num == 'HV0003') \\\n",
    "#     .take(5)  # or .head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de633b73",
   "metadata": {},
   "source": [
    "Why bother with the above when `SELECT * FROM df WHERE hvfhs_license_num = 'HV0003'` in SQL works?\n",
    "\n",
    "Spark is more flexible, and gives us **user-defined functions**\n",
    "\n",
    "But before we get into that, we can look at **Spark-provided functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e4f0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection of functions Spark already has\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1462fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # type in \"F.\" and hit TAB to see the list of functions\n",
    "# F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52ab256b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|pickup_date|dropoff_date|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+\n",
      "|           HV0005|              B02510|2021-01-02 11:31:29|2021-01-02 11:37:35|          28|         130|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0003|              B02877|2021-01-03 15:59:58|2021-01-03 16:13:50|         144|         261|   null| 2021-01-03|  2021-01-03|\n",
      "|           HV0005|              B02510|2021-01-02 20:41:20|2021-01-02 20:58:35|         138|         232|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0005|              B02510|2021-01-02 12:32:53|2021-01-02 12:37:51|          42|         116|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0003|              B02869|2021-01-01 14:39:29|2021-01-01 14:59:45|         148|          68|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0003|              B02880|2021-01-01 07:25:16|2021-01-01 07:50:46|          61|          76|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0003|              B02878|2021-01-02 01:05:28|2021-01-02 01:11:40|          42|          42|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0003|              B02887|2021-01-02 13:01:44|2021-01-02 13:25:23|         155|         177|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0003|              B02878|2021-01-01 05:51:46|2021-01-01 06:03:24|          49|         177|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0003|              B02765|2021-01-01 02:12:08|2021-01-01 02:19:49|          94|         174|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0003|              B02879|2021-01-01 02:17:17|2021-01-01 02:34:03|          42|           4|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0003|              B02764|2021-01-01 01:05:04|2021-01-01 01:17:42|         231|         265|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0003|              B02883|2021-01-03 01:05:38|2021-01-03 01:09:14|         229|         141|   null| 2021-01-03|  2021-01-03|\n",
      "|           HV0003|              B02880|2021-01-03 00:37:31|2021-01-03 01:01:18|         179|          14|   null| 2021-01-03|  2021-01-03|\n",
      "|           HV0003|              B02867|2021-01-01 17:23:04|2021-01-01 17:44:37|          76|          91|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0005|              B02510|2021-01-03 18:43:20|2021-01-03 18:54:21|           7|         129|   null| 2021-01-03|  2021-01-03|\n",
      "|           HV0003|              B02878|2021-01-01 21:10:25|2021-01-01 21:30:13|         263|          69|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0005|              B02510|2021-01-02 03:18:36|2021-01-02 03:42:58|         262|         198|   null| 2021-01-02|  2021-01-02|\n",
      "|           HV0003|              B02764|2021-01-01 14:22:54|2021-01-01 14:46:51|         229|          73|   null| 2021-01-01|  2021-01-01|\n",
      "|           HV0003|              B02395|2021-01-01 15:50:41|2021-01-01 15:56:03|         130|         130|   null| 2021-01-01|  2021-01-01|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # take a datetime and keep only the date\n",
    "# F.to_date()\n",
    "\n",
    "# add a new column to the dataframe\n",
    "df_spark \\\n",
    "    .withColumn('pickup_date', F.to_date(df_spark.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df_spark.dropoff_datetime)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48691bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-02|  2021-01-02|          28|         130|\n",
      "| 2021-01-03|  2021-01-03|         144|         261|\n",
      "| 2021-01-02|  2021-01-02|         138|         232|\n",
      "| 2021-01-02|  2021-01-02|          42|         116|\n",
      "| 2021-01-01|  2021-01-01|         148|          68|\n",
      "| 2021-01-01|  2021-01-01|          61|          76|\n",
      "| 2021-01-02|  2021-01-02|          42|          42|\n",
      "| 2021-01-02|  2021-01-02|         155|         177|\n",
      "| 2021-01-01|  2021-01-01|          49|         177|\n",
      "| 2021-01-01|  2021-01-01|          94|         174|\n",
      "| 2021-01-01|  2021-01-01|          42|           4|\n",
      "| 2021-01-01|  2021-01-01|         231|         265|\n",
      "| 2021-01-03|  2021-01-03|         229|         141|\n",
      "| 2021-01-03|  2021-01-03|         179|          14|\n",
      "| 2021-01-01|  2021-01-01|          76|          91|\n",
      "| 2021-01-03|  2021-01-03|           7|         129|\n",
      "| 2021-01-01|  2021-01-01|         263|          69|\n",
      "| 2021-01-02|  2021-01-02|         262|         198|\n",
      "| 2021-01-01|  2021-01-01|         229|          73|\n",
      "| 2021-01-01|  2021-01-01|         130|         130|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # take a datetime and keep only the date\n",
    "# F.to_date()\n",
    "\n",
    "# add a new column to the dataframe\n",
    "df_spark \\\n",
    "    .withColumn('pickup_date', F.to_date(df_spark.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df_spark.dropoff_datetime)) \\\n",
    "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcbc1dd",
   "metadata": {},
   "source": [
    "Again, we can also define our own functions\n",
    "\n",
    "This is not something we'd typically do in data warehouses, because it can be cumbersome\n",
    "\n",
    "But in PySpark, we can store all the code easily, cover it with tests, and really make sure the code works before executing it on you dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb8826d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-zoom]",
   "language": "python",
   "name": "conda-env-.conda-zoom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
