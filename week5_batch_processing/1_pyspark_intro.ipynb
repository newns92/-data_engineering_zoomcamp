{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9b60ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# instantiate a Spark session, an object that we use to interact with Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec744d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the zone data\n",
    "df_zone = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff44424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the fhv_tripdata_2021-01.csv.gz \n",
    "df_fhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b4c768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 33, 44), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 49, 7), PULocationID=230, DOLocationID=166, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 55, 19), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 18, 21), PULocationID=152, DOLocationID=167, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 23, 56), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 38, 5), PULocationID=233, DOLocationID=142, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 42, 51), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 45, 50), PULocationID=142, DOLocationID=143, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 48, 14), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 8, 42), PULocationID=143, DOLocationID=78, SR_Flag=None)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67cb736a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropoff_datetime', TimestampType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('SR_Flag', IntegerType(), True)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhv.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316da937",
   "metadata": {},
   "source": [
    "Keep only the first 1000 rows from the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "095fec35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ". : File C:\\Users\\nimz\\Documents\\WindowsPowerShell\\profile.ps1 cannot be loaded. The file \n",
      "C:\\Users\\nimz\\Documents\\WindowsPowerShell\\profile.ps1 is not digitally signed. You cannot run this script on the \n",
      "current system. For more information about running scripts and setting execution policy, see about_Execution_Policies \n",
      "at https:/go.microsoft.com/fwlink/?LinkID=135170.\n",
      "At line:1 char:3\n",
      "+ . 'C:\\Users\\nimz\\Documents\\WindowsPowerShell\\profile.ps1'\n",
      "+   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    + CategoryInfo          : SecurityError: (:) [], PSSecurityException\n",
      "    + FullyQualifiedErrorId : UnauthorizedAccess\n"
     ]
    }
   ],
   "source": [
    "# Keep only the first 1000 rows from the CSV\n",
    "# # BASH ONLY\n",
    "# !head -n 1001 fhvhv_tripdata_2021-01.csv > head.csv\n",
    "\n",
    "# first unzip the .GZ file with 7zip in Windows Explorer\n",
    "\n",
    "# https://www.reddit.com/r/excel/comments/99njm0/how_do_i_get_the_first_1000_rows_from_a_6_gb_csv/\n",
    "!powershell -command \"& {get-content fhvhv_tripdata_2021-01.csv|select-object -first 10}\" > head.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e9eb19",
   "metadata": {},
   "source": [
    "Pandas should read in data types very accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d55ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "577325fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8f1f3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a6ae0",
   "metadata": {},
   "source": [
    "**Turn this pandas dataframe into a Spark dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "781cd120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\python\\pyspark\\sql\\pandas\\conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "C:\\spark\\python\\pyspark\\sql\\pandas\\conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|    NaN|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|    NaN|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|    NaN|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|    NaN|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|    NaN|\n",
      "|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|    NaN|\n",
      "|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|    NaN|\n",
      "|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|    NaN|\n",
      "|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|    NaN|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a Spark dataframe from the pandas dataframe and view the schema\n",
    "spark.createDataFrame(df_pandas).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7e5432c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('SR_Flag', DoubleType(), True)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a Spark dataframe from the pandas dataframe and view the schema\n",
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c27fc",
   "metadata": {},
   "source": [
    "We want to change those `LongType` data types, which are less memory-efficient (use 8 bytes instead of the 4 bytes used by IntegerType), into Integers\n",
    "\n",
    "It is also possible to observe that `pickup_datetime` and `dropoff_datetime` were both interpreted as strings\n",
    "\n",
    "We want to achieve better performance and intepret all fields correctly, so we can specify the expected schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faf3bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5dd4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify expected schema, making sure the columsn can be nullable (that final \"True\" Boolean value)\n",
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acb7fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataframe with the expected schema\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "524e6fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 33, 44), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 49, 7), PULocationID=230, DOLocationID=166, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 55, 19), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 18, 21), PULocationID=152, DOLocationID=167, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 23, 56), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 38, 5), PULocationID=233, DOLocationID=142, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 42, 51), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 45, 50), PULocationID=142, DOLocationID=143, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 48, 14), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 8, 42), PULocationID=143, DOLocationID=78, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 6, 59), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 43, 1), PULocationID=88, DOLocationID=42, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 50), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 4, 57), PULocationID=42, DOLocationID=151, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 14, 30), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 50, 27), PULocationID=71, DOLocationID=226, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02875', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 22, 54), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 30, 20), PULocationID=112, DOLocationID=255, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02875', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 40, 12), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 53, 31), PULocationID=255, DOLocationID=232, SR_Flag=None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab896b",
   "metadata": {},
   "source": [
    "Right now our `fhvhv_tripdata_2021-01.csv` file is quite large\n",
    "\n",
    "We currently have a **Spark Cluster**, inside of which are a bunch of **executors** (machines doing computational work)\n",
    "- Let's say we have a bunch of files in our GCS Bucket containing a bunch of files\n",
    "- Each of these files will be pulled to an executor\n",
    "    - If we have more executors than files, Spark waits until one executor is done with a file and then that executor will grab one of the leftover files\n",
    "- Say there's only 1 large file and 6 executors\n",
    "    - One executor will grab this file, and the other 5 will be idle\n",
    "    - This is not efficient\n",
    "    - We'd rather have a bunch of smaller files\n",
    "- We can break up the large file in Spark via **partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5615a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the large \"file\" into 24 partitions\n",
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f1e7493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe (via its partitions)\n",
    "df.write.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cae26",
   "metadata": {},
   "source": [
    "**You will be able to see all 24 partitions as files in the `fhvhv/2021/01/`dir**\n",
    "\n",
    "**Can also see a `parquet` job at http://localhost:4040/jobs/ that we can click into and view more information such as DAGs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01cba3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 08A3-CF2D\n",
      "\n",
      " Directory of C:\\Users\\nimz\\Documents\\de_zoomcamp\\week5_batch_processing\\fhvhv\\2021\\01\n",
      "\n",
      "05/09/2023  08:47 PM    <DIR>          .\n",
      "05/09/2023  08:47 PM    <DIR>          ..\n",
      "05/09/2023  08:47 PM            71,484 .part-00000-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,468 .part-00001-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,476 .part-00002-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,480 .part-00003-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,480 .part-00004-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,484 .part-00005-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,444 .part-00006-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,480 .part-00007-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,464 .part-00008-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,468 .part-00009-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,456 .part-00010-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,464 .part-00011-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,500 .part-00012-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,476 .part-00013-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,512 .part-00014-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,468 .part-00015-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,524 .part-00016-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,488 .part-00017-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,508 .part-00018-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,472 .part-00019-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,492 .part-00020-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,444 .part-00021-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,460 .part-00022-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM            71,496 .part-00023-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet.crc\n",
      "05/09/2023  08:47 PM                 8 ._SUCCESS.crc\n",
      "05/09/2023  08:47 PM         9,148,662 part-00000-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,467 part-00001-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,147,559 part-00002-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,148,033 part-00003-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,148,117 part-00004-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,148,532 part-00005-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,143,335 part-00006-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,148,092 part-00007-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,261 part-00008-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,732 part-00009-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,145,061 part-00010-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,075 part-00011-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,150,733 part-00012-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,147,636 part-00013-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,152,105 part-00014-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,146,498 part-00015-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,153,804 part-00016-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,149,358 part-00017-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,151,965 part-00018-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,147,383 part-00019-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,149,612 part-00020-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,143,484 part-00021-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,145,646 part-00022-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM         9,150,453 part-00023-1fc571d4-0079-41cb-ad93-7b7dd5e2db6c-c000.snappy.parquet\n",
      "05/09/2023  08:47 PM                 0 _SUCCESS\n",
      "              50 File(s)    221,267,099 bytes\n",
      "               2 Dir(s)  368,523,157,504 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir fhvhv\\2021\\01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98557b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the partitioned files back into a Spark dataframe\n",
    "df = spark.read.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f871967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-zoom]",
   "language": "python",
   "name": "conda-env-.conda-zoom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
