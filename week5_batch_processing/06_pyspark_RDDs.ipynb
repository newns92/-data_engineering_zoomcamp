{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b07bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# instantiate a Spark session, an object that we use to interact with Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8c29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the green data\n",
    "df_green = spark.read.parquet('data/parquet/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "088536fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2020-01-22 13:18:32|  2020-01-22 13:45:58|                 N|         1|         244|          41|              1|         5.22|       22.0|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|        22.8|           1|        1|                 0.0|\n",
      "|       2| 2020-01-23 17:54:10|  2020-01-23 18:01:02|                 N|         1|         236|         263|              1|         0.87|        6.5|  1.0|    0.5|       1.2|         0.0|     null|                  0.3|       12.25|           1|        1|                2.75|\n",
      "|       2| 2020-01-19 10:23:37|  2020-01-19 10:26:29|                 N|         1|         166|         166|              4|         0.63|        4.5|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|         5.3|           2|        1|                 0.0|\n",
      "|       2| 2020-01-21 14:25:16|  2020-01-21 14:35:51|                 N|         1|         152|         238|              1|         2.71|       11.0|  0.0|    0.5|      2.91|         0.0|     null|                  0.3|       17.46|           1|        1|                2.75|\n",
      "|    null| 2020-01-07 09:46:00|  2020-01-07 09:59:00|              null|      null|          51|           3|           null|         2.13|      18.96| 2.75|    0.0|       0.0|         0.0|     null|                  0.3|       22.01|        null|     null|                null|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c48aab",
   "metadata": {},
   "source": [
    "**We will be implementing the following SQL query:**\n",
    "```bash\n",
    "    SELECT \n",
    "        date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "        PULocationID AS zone,\n",
    "\n",
    "        SUM(total_amount) AS amount,\n",
    "        COUNT(1) AS number_records\n",
    "    FROM\n",
    "        green\n",
    "    WHERE\n",
    "        lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "    GROUP BY\n",
    "        1, 2\n",
    "```\n",
    "\n",
    "**First we load in the green data into a Spark RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a69fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert DataFrame into an RDD with select columns\n",
    "rdd = df_green \\\n",
    "    .select('lpep_pickup_datetime', 'PULocationID', 'total_amount') \\\n",
    "    .rdd  # this is the underlying RDD of our current DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ade7cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 22, 13, 18, 32), PULocationID=244, total_amount=22.8),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 17, 54, 10), PULocationID=236, total_amount=12.25),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 19, 10, 23, 37), PULocationID=166, total_amount=5.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 21, 14, 25, 16), PULocationID=152, total_amount=17.46),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 7, 9, 46), PULocationID=51, total_amount=22.01)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"take\" (look at) the first 5 rows in the rdd\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ff2c7",
   "metadata": {},
   "source": [
    "**Above, note that the RDD is made up of `Row` objects are used to build DataFrames in Spark**\n",
    "\n",
    "### 5 Basic Operations on RDDs: `filter`, `map`, `reduceByKey`, `map`, `toDF`\n",
    "- `.filter()` = we don’t want outliers\n",
    "- `.map()` = generates intermediate results better suited for aggregation\n",
    "- `.reduceByKey()` = merges the values for each key\n",
    "- `.map()` = unwraps the rows\n",
    "- `.toDF()` = returns the rows to a DataFrame format properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e07107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 22, 13, 18, 32), PULocationID=244, total_amount=22.8)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don’t want outliers and only need trips from 1/1/2020, so we create a filter\n",
    "# i.e., implement a WHERE clause\n",
    "# .filter([define a function])\n",
    "\n",
    "# Lambda function = a small anonymous function.\n",
    "#   - can take any number of arguments, but can only have 1 expression\n",
    "#   - i.e., `lambda arguments : expression`\n",
    "#   - ex: Add 10 to argument a, and return the result: \n",
    "#     - x = lambda a : a + 10\n",
    "#     - print(x(5))\n",
    "\n",
    "# return the first row of the filtered data\n",
    "rdd.filter(lambda row: True).take(1)\n",
    "# rdd.filter(lambda row: False).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7970e872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more lambda testing\n",
    "\n",
    "# This returns even numbers.\n",
    "# See https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.RDD.filter.html\n",
    "#   and https://cumsum.wordpress.com/2019/10/12/typeerror-missing-1-required-positional-argument-y/\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "rdd = sc.parallelize(c=[1, 2, 3, 4, 5])\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15713dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake the RDD\n",
    "# convert DataFrame into an RDD with select columns\n",
    "rdd = df_green \\\n",
    "    .select('lpep_pickup_datetime', 'PULocationID', 'total_amount') \\\n",
    "    .rdd  # this is the underlying RDD of our current DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35427fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to filter a Timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "start = datetime(year=2020, month=1, day=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e6b6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 22, 13, 18, 32), PULocationID=244, total_amount=22.8),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 17, 54, 10), PULocationID=236, total_amount=12.25),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 19, 10, 23, 37), PULocationID=166, total_amount=5.3)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter based on this start date, get the first 3 rows (WHERE clause)\n",
    "rdd.filter(lambda row: row.lpep_pickup_datetime >= start).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "768c3eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 22, 13, 18, 32), PULocationID=244, total_amount=22.8),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 17, 54, 10), PULocationID=236, total_amount=12.25),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 19, 10, 23, 37), PULocationID=166, total_amount=5.3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the above logic into a function because lambdas get messy quite fast\n",
    "def filter_outliers(row):\n",
    "    return row.lpep_pickup_datetime >= start\n",
    "\n",
    "# use the function\n",
    "rdd.filter(filter_outliers).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91bcca8",
   "metadata": {},
   "source": [
    "**To implement the equivalent of GROUP BY, we need the `.map()` function, which when applied a *transformation to every `Row`* returns a *transformed RDD***\n",
    "- `.filter()` *alone* returns `True` or `False`\n",
    "- `.map()` is applied to every element of the RDD\n",
    "    - In our case, it recieves a `Row` object as input and outputs something else\n",
    "\n",
    "**We create a function to use withing `.map()` that transforms a row by creating a *tuple* composed of a key and a value**\n",
    "- The key = a tuple of `hour` and `zone`, the same 2 columns of the GROUP BY\n",
    "- The value is a tuple of `amount` and `number_of_records`, the same 2 columns that are returned by the SQL query above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f631579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply .map() to implement SUM(total_amount) AS amount and COUNT(1) AS number_records\n",
    "# In order to be able to GROUP BY, output of .map() must be something in the format (key, value)\n",
    "#   - where key = a composite key [hour zone] and value will be [amount, number_of_trips]:\n",
    "def prepare_for_grouping(row):\n",
    "    # .replace() returns a new DataFrame replacing a value with another value\n",
    "    # we replace every value more granula than day EXCEPT hour with value 0\n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    # create the key tuple\n",
    "    key = (hour, zone)\n",
    "    \n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount, count)\n",
    "    \n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9c8bfe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((datetime.datetime(2020, 1, 22, 13, 0), 244), (22.8, 1)),\n",
       " ((datetime.datetime(2020, 1, 23, 17, 0), 236), (12.25, 1)),\n",
       " ((datetime.datetime(2020, 1, 19, 10, 0), 166), (5.3, 1)),\n",
       " ((datetime.datetime(2020, 1, 21, 14, 0), 152), (17.46, 1)),\n",
       " ((datetime.datetime(2020, 1, 7, 9, 0), 51), (22.01, 1))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply our new function to GROUP BY\n",
    "rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945ac84b",
   "metadata": {},
   "source": [
    "**Next, we will aggregate this RDD transformed RDD by the key (combine values with the same key into a single record)**\n",
    "- **i.e., perform a REDUCE operation**\n",
    "\n",
    "**We use `pyspark.RDD.reduceByKey()` to merge the values for each key using an associative and commutative REDUCE function**\n",
    "- i.e., it takes elements with the same key and merges them together\n",
    "- Where `left_value` and `right_value` are tuple of `amount` and `number_of_records`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db4ce3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the aggregation/REDUCE function/operation on `amount` and `number_of_records`\n",
    "def calculate_revenue(left_value, right_value):\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "\n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "\n",
    "    return (output_amount, output_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e4b8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the REDUCE operation to our RDD\n",
    "df_result = rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue)  # perform a REDUCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f72d91b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((datetime.datetime(2020, 1, 19, 10, 0), 166), (969.2699999999993, 55)),\n",
       " ((datetime.datetime(2020, 1, 24, 17, 0), 18), (26.26, 1)),\n",
       " ((datetime.datetime(2020, 1, 21, 13, 0), 33), (535.8900000000001, 26)),\n",
       " ((datetime.datetime(2020, 1, 15, 21, 0), 42), (336.1400000000001, 23)),\n",
       " ((datetime.datetime(2020, 1, 4, 12, 0), 116), (119.85, 8))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fce28d",
   "metadata": {},
   "source": [
    "**This nested structure seen right above (nested tuples/`Row`'s) is not too intuitive nor easy to manipulate, so we want to turn it back into a DataFrame**\n",
    "\n",
    "**To do so, we apply another `.map()` function to apply a function to each element (`Row`) of the RDD and transform the `Row`'s into desired DataFrame columns (i.e., unwrap this structure)**\n",
    "\n",
    "**We first use a `namedtuple` to preserve columns names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14499549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# returns a new subclass of tuple with named fields\n",
    "RevenueRow = namedtuple(typename='RevenueRow', field_names=['hour', 'zone', 'revenue', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e603db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap(row):\n",
    "    return RevenueRow(\n",
    "        hour=row[0][0], \n",
    "        zone=row[0][1],\n",
    "        revenue=row[1][0],\n",
    "        count=row[1][1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa23364",
   "metadata": {},
   "source": [
    "**To return to a DataFrame properly, we want to specify the schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02c0fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result = rdd \\\n",
    "#     .filter(filter_outliers) \\\n",
    "#     .map(prepare_for_grouping) \\\n",
    "#     .reduceByKey(calculate_revenue) \\\n",
    "#     .map(unwrap) \\\n",
    "#     .toDF()  # return the RDD as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2f56027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cba02978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "result_schema = types.StructType([\n",
    "    types.StructField('hour', types.TimestampType(), True),\n",
    "    types.StructField('zone', types.IntegerType(), True),\n",
    "    types.StructField('revenue', types.DoubleType(), True),\n",
    "    types.StructField('count', types.IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11619426",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF(result_schema)  # return the RDD as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "762f45a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----------------+-----+\n",
      "|               hour|zone|          revenue|count|\n",
      "+-------------------+----+-----------------+-----+\n",
      "|2020-01-19 10:00:00| 166|969.2699999999993|   55|\n",
      "|2020-01-24 17:00:00|  18|            26.26|    1|\n",
      "|2020-01-21 13:00:00|  33|535.8900000000001|   26|\n",
      "|2020-01-15 21:00:00|  42|336.1400000000001|   23|\n",
      "|2020-01-04 12:00:00| 116|           119.85|    8|\n",
      "+-------------------+----+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b6c35fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the resulting DataFrame to a temp parquet file\n",
    "df_result.write.parquet('tmp/green-revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f36b69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 08A3-CF2D\n",
      "\n",
      " Directory of C:\\Users\\nimz\\Documents\\de_zoomcamp\\week5_batch_processing\\tmp\n",
      "\n",
      "05/11/2023  07:54 PM    <DIR>          .\n",
      "05/11/2023  07:54 PM    <DIR>          ..\n",
      "05/11/2023  07:54 PM    <DIR>          green-revenue\n",
      "05/11/2023  06:42 PM    <DIR>          revenue-zones\n",
      "               0 File(s)              0 bytes\n",
      "               4 Dir(s)  368,424,480,768 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir tmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-zoom]",
   "language": "python",
   "name": "conda-env-.conda-zoom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
