{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b92ab0c",
   "metadata": {},
   "source": [
    "# Get the Data\n",
    "\n",
    "Run `download_data.sh` in a Git MINGW64 terminal to get all the data files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00be3d",
   "metadata": {},
   "source": [
    "# Start the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eb2286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create the SparkSession locally with as many CPUs cores as possible\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2396ea2",
   "metadata": {},
   "source": [
    "**Open the Spark UI at http://localhost:4040/**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9432b6c",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "700929d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>ehail_fee</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-12-18 15:52:30</td>\n",
       "      <td>2019-12-18 15:54:39</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-01 00:45:58</td>\n",
       "      <td>2020-01-01 00:56:39</td>\n",
       "      <td>N</td>\n",
       "      <td>5</td>\n",
       "      <td>66</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>1.28</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>24.36</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-01 00:41:38</td>\n",
       "      <td>2020-01-01 00:52:49</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>181</td>\n",
       "      <td>228</td>\n",
       "      <td>1</td>\n",
       "      <td>2.47</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>15.34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-01 00:52:46</td>\n",
       "      <td>2020-01-01 01:14:21</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>129</td>\n",
       "      <td>263</td>\n",
       "      <td>2</td>\n",
       "      <td>6.30</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.05</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-01 00:19:57</td>\n",
       "      <td>2020-01-01 00:30:56</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>210</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>2.30</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID lpep_pickup_datetime lpep_dropoff_datetime store_and_fwd_flag  \\\n",
       "0         2  2019-12-18 15:52:30   2019-12-18 15:54:39                  N   \n",
       "1         2  2020-01-01 00:45:58   2020-01-01 00:56:39                  N   \n",
       "2         2  2020-01-01 00:41:38   2020-01-01 00:52:49                  N   \n",
       "3         1  2020-01-01 00:52:46   2020-01-01 01:14:21                  N   \n",
       "4         1  2020-01-01 00:19:57   2020-01-01 00:30:56                  N   \n",
       "\n",
       "   RatecodeID  PULocationID  DOLocationID  passenger_count  trip_distance  \\\n",
       "0           1           264           264                5           0.00   \n",
       "1           5            66            65                2           1.28   \n",
       "2           1           181           228                1           2.47   \n",
       "3           1           129           263                2           6.30   \n",
       "4           1           210           150                1           2.30   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  ehail_fee  \\\n",
       "0          3.5   0.50      0.5        0.01           0.0        NaN   \n",
       "1         20.0   0.00      0.0        4.06           0.0        NaN   \n",
       "2         10.5   0.50      0.5        3.54           0.0        NaN   \n",
       "3         21.0   3.25      0.5        0.00           0.0        NaN   \n",
       "4         10.0   0.50      0.5        0.00           0.0        NaN   \n",
       "\n",
       "   improvement_surcharge  total_amount  payment_type  trip_type  \\\n",
       "0                    0.3          4.81             1          1   \n",
       "1                    0.3         24.36             1          2   \n",
       "2                    0.3         15.34             1          1   \n",
       "3                    0.3         25.05             2          1   \n",
       "4                    0.3         11.30             1          1   \n",
       "\n",
       "   congestion_surcharge  \n",
       "0                  0.00  \n",
       "1                  0.00  \n",
       "2                  0.00  \n",
       "3                  2.75  \n",
       "4                  0.00  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in Jan. 2020 green taxi data CSV into a pandas DataFrame\n",
    "df_green_pd = pd.read_csv('./data/raw/green/2020/01/green_tripdata_2020_01.csv.gz', nrows=1000, compression='gzip')\n",
    "\n",
    "df_green_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3288a52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                   int64\n",
       "lpep_pickup_datetime      object\n",
       "lpep_dropoff_datetime     object\n",
       "store_and_fwd_flag        object\n",
       "RatecodeID                 int64\n",
       "PULocationID               int64\n",
       "DOLocationID               int64\n",
       "passenger_count            int64\n",
       "trip_distance            float64\n",
       "fare_amount              float64\n",
       "extra                    float64\n",
       "mta_tax                  float64\n",
       "tip_amount               float64\n",
       "tolls_amount             float64\n",
       "ehail_fee                float64\n",
       "improvement_surcharge    float64\n",
       "total_amount             float64\n",
       "payment_type               int64\n",
       "trip_type                  int64\n",
       "congestion_surcharge     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green_pd.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe11c10",
   "metadata": {},
   "source": [
    "# Convert to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47d4d233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\python\\pyspark\\sql\\pandas\\conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "C:\\spark\\python\\pyspark\\sql\\pandas\\conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('lpep_pickup_datetime', StringType(), True), StructField('lpep_dropoff_datetime', StringType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', LongType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('passenger_count', LongType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', LongType(), True), StructField('trip_type', LongType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn pandas DataFrame into a Spark DataFrame\n",
    "spark.createDataFrame(df_green_pd).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04f4bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "# Define the green taxi schema\n",
    "green_schema = types.StructType([\n",
    "    types.StructField('VendorID', types.IntegerType(), True),\n",
    "    types.StructField('lpep_pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('lpep_dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('store_and_fwd_flag', types.StringType(), True),\n",
    "    types.StructField('RatecodeID', types.IntegerType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('passenger_count', types.IntegerType(), True),\n",
    "    types.StructField('trip_distance', types.DoubleType(), True),\n",
    "    types.StructField('fare_amount', types.DoubleType(), True),\n",
    "    types.StructField('extra', types.DoubleType(), True),\n",
    "    types.StructField('mta_tax', types.DoubleType(), True),\n",
    "    types.StructField('tip_amount', types.DoubleType(), True),\n",
    "    types.StructField('tolls_amount', types.DoubleType(), True),\n",
    "    types.StructField('ehail_fee', types.DoubleType(), True),\n",
    "    types.StructField('improvement_surcharge', types.DoubleType(), True),\n",
    "    types.StructField('total_amount', types.DoubleType(), True),\n",
    "    types.StructField('payment_type', types.IntegerType(), True),\n",
    "    types.StructField('trip_type', types.IntegerType(), True),\n",
    "    types.StructField('congestion_surcharge', types.DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cce7ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_spark = spark.read \\\n",
    "                .option('header', 'true') \\\n",
    "                .schema(green_schema) \\\n",
    "                .csv('./data/raw/green/2020/01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c93c37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ba453cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for yellow taxi data\n",
    "\n",
    "# Define the yellow taxi schema\n",
    "yellow_schema = types.StructType([\n",
    "    types.StructField('VendorID', types.IntegerType(), True),\n",
    "    types.StructField('tpep_pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('tpep_dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('passenger_count', types.IntegerType(), True),\n",
    "    types.StructField('trip_distance', types.DoubleType(), True),\n",
    "    types.StructField('RatecodeID', types.IntegerType(), True),\n",
    "    types.StructField('store_and_fwd_flag', types.StringType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('payment_type', types.IntegerType(), True),\n",
    "    types.StructField('fare_amount', types.DoubleType(), True),\n",
    "    types.StructField('extra', types.DoubleType(), True),\n",
    "    types.StructField('mta_tax', types.DoubleType(), True),\n",
    "    types.StructField('tip_amount', types.DoubleType(), True),\n",
    "    types.StructField('tolls_amount', types.DoubleType(), True),\n",
    "    types.StructField('improvement_surcharge', types.DoubleType(), True),\n",
    "    types.StructField('total_amount', types.DoubleType(), True),\n",
    "    types.StructField('congestion_surcharge', types.DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3e1b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_spark = spark.read \\\n",
    "                .option('header', 'true') \\\n",
    "                .schema(yellow_schema) \\\n",
    "                .csv('./data/raw/yellow/2020/01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcf83f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yellow_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9f7b7",
   "metadata": {},
   "source": [
    "# Write to Parquet Files\n",
    "\n",
    "**We do this so that the Parquet files store the schema that we have written above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "524a37ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021/01\n",
      "Processing data for 2021/02\n",
      "Processing data for 2021/03\n",
      "Processing data for 2021/04\n",
      "Processing data for 2021/05\n",
      "Processing data for 2021/06\n",
      "Processing data for 2021/07\n",
      "Processing data for 2021/08\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/C:/Users/nimz/Documents/de_zoomcamp/week5_batch_processing/data/raw/green/2021/08",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/raw/green/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/pq/green/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 11\u001b[0m df_green_spark \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgreen_schema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m     16\u001b[0m df_green_spark\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(output_path)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\readwriter.py:535\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Path does not exist: file:/C:/Users/nimz/Documents/de_zoomcamp/week5_batch_processing/data/raw/green/2021/08"
     ]
    }
   ],
   "source": [
    "# Repartition the data file into 4 partitions into Parquet files\n",
    "year = 2021\n",
    "\n",
    "for month in range(1, 13):\n",
    "    \n",
    "    print(f'Processing data for {year}/{month:02d}')\n",
    "    \n",
    "    input_path = f'./data/raw/green/{year}/{month:02d}/'\n",
    "    output_path = f'./data/pq/green/{year}/{month:02d}/'\n",
    "    \n",
    "    df_green_spark = spark.read \\\n",
    "                    .option('header', 'true') \\\n",
    "                    .schema(green_schema) \\\n",
    "                    .csv(input_path)    \n",
    "    \n",
    "    df_green_spark.repartition(4).write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd2da21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2020/01\n",
      "Processing data for 2020/02\n",
      "Processing data for 2020/03\n",
      "Processing data for 2020/04\n",
      "Processing data for 2020/05\n",
      "Processing data for 2020/06\n",
      "Processing data for 2020/07\n",
      "Processing data for 2020/08\n",
      "Processing data for 2020/09\n",
      "Processing data for 2020/10\n",
      "Processing data for 2020/11\n",
      "Processing data for 2020/12\n"
     ]
    }
   ],
   "source": [
    "# Repartition the data file into 4 partitions into Parquet files\n",
    "year = 2020\n",
    "\n",
    "for month in range(1, 13):\n",
    "    \n",
    "    print(f'Processing data for {year}/{month:02d}')\n",
    "    \n",
    "    input_path = f'./data/raw/green/{year}/{month:02d}/'\n",
    "    output_path = f'./data/pq/green/{year}/{month:02d}/'\n",
    "    \n",
    "    df_green_spark = spark.read \\\n",
    "                    .option('header', 'true') \\\n",
    "                    .schema(green_schema) \\\n",
    "                    .csv(input_path)    \n",
    "    \n",
    "    df_green_spark.repartition(4).write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c84af290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021/01\n",
      "Processing data for 2021/02\n",
      "Processing data for 2021/03\n",
      "Processing data for 2021/04\n",
      "Processing data for 2021/05\n",
      "Processing data for 2021/06\n",
      "Processing data for 2021/07\n",
      "Processing data for 2021/08\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/C:/Users/nimz/Documents/de_zoomcamp/week5_batch_processing/data/raw/yellow/2021/08",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/raw/yellow/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/pq/yellow/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 11\u001b[0m df_green_spark \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgreen_schema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m     16\u001b[0m df_green_spark\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(output_path)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\readwriter.py:535\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Path does not exist: file:/C:/Users/nimz/Documents/de_zoomcamp/week5_batch_processing/data/raw/yellow/2021/08"
     ]
    }
   ],
   "source": [
    "# Repartition the data file into 4 partitions into Parquet files\n",
    "year = 2021\n",
    "\n",
    "for month in range(1, 13):\n",
    "    \n",
    "    print(f'Processing data for {year}/{month:02d}')\n",
    "    \n",
    "    input_path = f'./data/raw/yellow/{year}/{month:02d}/'\n",
    "    output_path = f'./data/pq/yellow/{year}/{month:02d}/'\n",
    "    \n",
    "    df_green_spark = spark.read \\\n",
    "                    .option('header', 'true') \\\n",
    "                    .schema(green_schema) \\\n",
    "                    .csv(input_path)    \n",
    "    \n",
    "    df_green_spark.repartition(4).write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad852136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2020/01\n",
      "Processing data for 2020/02\n",
      "Processing data for 2020/03\n",
      "Processing data for 2020/04\n",
      "Processing data for 2020/05\n",
      "Processing data for 2020/06\n",
      "Processing data for 2020/07\n",
      "Processing data for 2020/08\n",
      "Processing data for 2020/09\n",
      "Processing data for 2020/10\n",
      "Processing data for 2020/11\n",
      "Processing data for 2020/12\n"
     ]
    }
   ],
   "source": [
    "# Repartition the data file into 4 partitions into Parquet files\n",
    "year = 2020\n",
    "\n",
    "for month in range(1, 13):\n",
    "    \n",
    "    print(f'Processing data for {year}/{month:02d}')\n",
    "    \n",
    "    input_path = f'./data/raw/yellow/{year}/{month:02d}/'\n",
    "    output_path = f'./data/pq/yellow/{year}/{month:02d}/'\n",
    "    \n",
    "    df_green_spark = spark.read \\\n",
    "                    .option('header', 'true') \\\n",
    "                    .schema(green_schema) \\\n",
    "                    .csv(input_path)    \n",
    "    \n",
    "    df_green_spark.repartition(4).write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd87093",
   "metadata": {},
   "source": [
    "# Use the Python Script\n",
    "\n",
    "Instead of doing all of the above, run `download_data.sh` in a Git MINGW64 terminal to get all the data files, and then in a `zoom` Conda environment, run `create_taxi_schema.py` to get all the data into the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bdc297a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/27 20:53:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/02/27 20:53:54 WARN ZlibFactory: Failed to load/initialize native-zlib library\n",
      "24/02/27 20:53:59 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\n",
      "Creating the SparkSession...\n",
      "Creating the schemas...\n",
      "Processing data for yellow/2020/1\n",
      "Reading data/raw/yellow/2020/01/\n",
      "Partitioning and saving data/raw/yellow/2020/01/ to data/parquet/yellow/2020/01/\n",
      "Processing data for yellow/2020/2\n",
      "Reading data/raw/yellow/2020/02/\n",
      "Partitioning and saving data/raw/yellow/2020/02/ to data/parquet/yellow/2020/02/\n",
      "Processing data for yellow/2020/3\n",
      "Reading data/raw/yellow/2020/03/\n",
      "Partitioning and saving data/raw/yellow/2020/03/ to data/parquet/yellow/2020/03/\n",
      "Processing data for yellow/2020/4\n",
      "Reading data/raw/yellow/2020/04/\n",
      "Partitioning and saving data/raw/yellow/2020/04/ to data/parquet/yellow/2020/04/\n",
      "Processing data for yellow/2020/5\n",
      "Reading data/raw/yellow/2020/05/\n",
      "Partitioning and saving data/raw/yellow/2020/05/ to data/parquet/yellow/2020/05/\n",
      "Processing data for yellow/2020/6\n",
      "Reading data/raw/yellow/2020/06/\n",
      "Partitioning and saving data/raw/yellow/2020/06/ to data/parquet/yellow/2020/06/\n",
      "Processing data for yellow/2020/7\n",
      "Reading data/raw/yellow/2020/07/\n",
      "Partitioning and saving data/raw/yellow/2020/07/ to data/parquet/yellow/2020/07/\n",
      "Processing data for yellow/2020/8\n",
      "Reading data/raw/yellow/2020/08/\n",
      "Partitioning and saving data/raw/yellow/2020/08/ to data/parquet/yellow/2020/08/\n",
      "Processing data for yellow/2020/9\n",
      "Reading data/raw/yellow/2020/09/\n",
      "Partitioning and saving data/raw/yellow/2020/09/ to data/parquet/yellow/2020/09/\n",
      "Processing data for yellow/2020/10\n",
      "Reading data/raw/yellow/2020/10/\n",
      "Partitioning and saving data/raw/yellow/2020/10/ to data/parquet/yellow/2020/10/\n",
      "Processing data for yellow/2020/11\n",
      "Reading data/raw/yellow/2020/11/\n",
      "Partitioning and saving data/raw/yellow/2020/11/ to data/parquet/yellow/2020/11/\n",
      "Processing data for yellow/2020/12\n",
      "Reading data/raw/yellow/2020/12/\n",
      "Partitioning and saving data/raw/yellow/2020/12/ to data/parquet/yellow/2020/12/\n",
      "Processing data for yellow/2021/1\n",
      "Reading data/raw/yellow/2021/01/\n",
      "Partitioning and saving data/raw/yellow/2021/01/ to data/parquet/yellow/2021/01/\n",
      "Processing data for yellow/2021/2\n",
      "Reading data/raw/yellow/2021/02/\n",
      "Partitioning and saving data/raw/yellow/2021/02/ to data/parquet/yellow/2021/02/\n",
      "Processing data for yellow/2021/3\n",
      "Reading data/raw/yellow/2021/03/\n",
      "Partitioning and saving data/raw/yellow/2021/03/ to data/parquet/yellow/2021/03/\n",
      "Processing data for yellow/2021/4\n",
      "Reading data/raw/yellow/2021/04/\n",
      "Partitioning and saving data/raw/yellow/2021/04/ to data/parquet/yellow/2021/04/\n",
      "Processing data for yellow/2021/5\n",
      "Reading data/raw/yellow/2021/05/\n",
      "Partitioning and saving data/raw/yellow/2021/05/ to data/parquet/yellow/2021/05/\n",
      "Processing data for yellow/2021/6\n",
      "Reading data/raw/yellow/2021/06/\n",
      "Partitioning and saving data/raw/yellow/2021/06/ to data/parquet/yellow/2021/06/\n",
      "Processing data for yellow/2021/7\n",
      "Reading data/raw/yellow/2021/07/\n",
      "Partitioning and saving data/raw/yellow/2021/07/ to data/parquet/yellow/2021/07/\n",
      "Processing data for green/2020/1\n",
      "Reading data/raw/green/2020/01/\n",
      "Partitioning and saving data/raw/green/2020/01/ to data/parquet/green/2020/01/\n",
      "Processing data for green/2020/2\n",
      "Reading data/raw/green/2020/02/\n",
      "Partitioning and saving data/raw/green/2020/02/ to data/parquet/green/2020/02/\n",
      "Processing data for green/2020/3\n",
      "Reading data/raw/green/2020/03/\n",
      "Partitioning and saving data/raw/green/2020/03/ to data/parquet/green/2020/03/\n",
      "Processing data for green/2020/4\n",
      "Reading data/raw/green/2020/04/\n",
      "Partitioning and saving data/raw/green/2020/04/ to data/parquet/green/2020/04/\n",
      "Processing data for green/2020/5\n",
      "Reading data/raw/green/2020/05/\n",
      "Partitioning and saving data/raw/green/2020/05/ to data/parquet/green/2020/05/\n",
      "Processing data for green/2020/6\n",
      "Reading data/raw/green/2020/06/\n",
      "Partitioning and saving data/raw/green/2020/06/ to data/parquet/green/2020/06/\n",
      "Processing data for green/2020/7\n",
      "Reading data/raw/green/2020/07/\n",
      "Partitioning and saving data/raw/green/2020/07/ to data/parquet/green/2020/07/\n",
      "Processing data for green/2020/8\n",
      "Reading data/raw/green/2020/08/\n",
      "Partitioning and saving data/raw/green/2020/08/ to data/parquet/green/2020/08/\n",
      "Processing data for green/2020/9\n",
      "Reading data/raw/green/2020/09/\n",
      "Partitioning and saving data/raw/green/2020/09/ to data/parquet/green/2020/09/\n",
      "Processing data for green/2020/10\n",
      "Reading data/raw/green/2020/10/\n",
      "Partitioning and saving data/raw/green/2020/10/ to data/parquet/green/2020/10/\n",
      "Processing data for green/2020/11\n",
      "Reading data/raw/green/2020/11/\n",
      "Partitioning and saving data/raw/green/2020/11/ to data/parquet/green/2020/11/\n",
      "Processing data for green/2020/12\n",
      "Reading data/raw/green/2020/12/\n",
      "Partitioning and saving data/raw/green/2020/12/ to data/parquet/green/2020/12/\n",
      "Processing data for green/2021/1\n",
      "Reading data/raw/green/2021/01/\n",
      "Partitioning and saving data/raw/green/2021/01/ to data/parquet/green/2021/01/\n",
      "Processing data for green/2021/2\n",
      "Reading data/raw/green/2021/02/\n",
      "Partitioning and saving data/raw/green/2021/02/ to data/parquet/green/2021/02/\n",
      "Processing data for green/2021/3\n",
      "Reading data/raw/green/2021/03/\n",
      "Partitioning and saving data/raw/green/2021/03/ to data/parquet/green/2021/03/\n",
      "Processing data for green/2021/4\n",
      "Reading data/raw/green/2021/04/\n",
      "Partitioning and saving data/raw/green/2021/04/ to data/parquet/green/2021/04/\n",
      "Processing data for green/2021/5\n",
      "Reading data/raw/green/2021/05/\n",
      "Partitioning and saving data/raw/green/2021/05/ to data/parquet/green/2021/05/\n",
      "Processing data for green/2021/6\n",
      "Reading data/raw/green/2021/06/\n",
      "Partitioning and saving data/raw/green/2021/06/ to data/parquet/green/2021/06/\n",
      "Processing data for green/2021/7\n",
      "Reading data/raw/green/2021/07/\n",
      "Partitioning and saving data/raw/green/2021/07/ to data/parquet/green/2021/07/\n",
      "SUCCESS: The process with PID 35792 (child process of PID 13800) has been terminated.\n",
      "SUCCESS: The process with PID 13800 (child process of PID 28668) has been terminated.\n",
      "SUCCESS: The process with PID 28668 (child process of PID 26088) has been terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\n",
      "\n",
      "[Stage 2:>                                                          (0 + 4) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 3:>                                                          (0 + 1) / 1]\n",
      "\n",
      "[Stage 5:>                                                          (0 + 4) / 4]\n",
      "\n",
      "[Stage 5:==============>                                            (1 + 3) / 4]\n",
      "\n",
      "[Stage 5:============================================>              (3 + 1) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 6:>                                                          (0 + 1) / 1]\n",
      "\n",
      "[Stage 8:>                                                          (0 + 4) / 4]\n",
      "\n",
      "[Stage 8:=============================>                             (2 + 2) / 4]\n",
      "\n",
      "[Stage 8:============================================>              (3 + 1) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 12:>                                                         (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 15:>                                                         (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 18:>                                                         (0 + 1) / 1]\n",
      "\n",
      "[Stage 20:>                                                         (0 + 4) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 21:>                                                         (0 + 1) / 1]\n",
      "\n",
      "[Stage 23:>                                                         (0 + 4) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 24:>                                                         (0 + 1) / 1]\n",
      "\n",
      "[Stage 26:>                                                         (0 + 4) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 27:>                                                         (0 + 1) / 1]\n",
      "\n",
      "[Stage 29:>                                                         (0 + 4) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 30:>                                                         (0 + 1) / 1]\n",
      "\n",
      "[Stage 32:>                                                         (0 + 4) / 4]\n",
      "\n",
      "[Stage 32:==============>                                           (1 + 3) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 33:>                                                         (0 + 1) / 1]\n",
      "\n",
      "[Stage 35:>                                                         (0 + 4) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 36:>                                                         (0 + 1) / 1]\n",
      "\n",
      "[Stage 38:>                                                         (0 + 4) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 39:>                                                         (0 + 1) / 1]\n",
      "\n",
      "[Stage 41:>                                                         (0 + 4) / 4]\n",
      "\n",
      "[Stage 41:==============>                                           (1 + 3) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 42:>                                                         (0 + 1) / 1]\n",
      "\n",
      "[Stage 44:>                                                         (0 + 4) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 45:>                                                         (0 + 1) / 1]\n",
      "\n",
      "[Stage 47:>                                                         (0 + 4) / 4]\n",
      "\n",
      "[Stage 47:==============>                                           (1 + 3) / 4]\n",
      "\n",
      "[Stage 47:=============================>                            (2 + 2) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 48:>                                                         (0 + 1) / 1]\n",
      "\n",
      "[Stage 50:>                                                         (0 + 4) / 4]\n",
      "\n",
      "[Stage 50:===========================================>              (3 + 1) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 51:>                                                         (0 + 1) / 1]\n",
      "\n",
      "[Stage 53:>                                                         (0 + 4) / 4]\n",
      "\n",
      "[Stage 53:==============>                                           (1 + 3) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 54:>                                                         (0 + 1) / 1]\n",
      "\n",
      "[Stage 56:>                                                         (0 + 4) / 4]\n",
      "\n",
      "[Stage 56:==============>                                           (1 + 3) / 4]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 57:>                                                         (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 60:>                                                         (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 63:>                                                         (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n"
     ]
    }
   ],
   "source": [
    "!python create_taxi_schema.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a0293ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(VendorID=2, pickup_datetime=datetime.datetime(2020, 1, 22, 13, 18, 32), dropoff_datetime=datetime.datetime(2020, 1, 22, 13, 45, 58), store_and_fwd_flag='N', RatecodeID=1, PULocationID=244, DOLocationID=41, passenger_count=1, trip_distance=5.22, fare_amount=22.0, extra=0.0, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=22.8, payment_type=1, trip_type=1, congestion_surcharge=0.0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in all green data into a Spark DataFrame\n",
    "df_green = spark.read.parquet('./data/parquet/green/*/*')\n",
    "\n",
    "# Rename some columns via PySpark function to match up with yellow (will also be renamed)\n",
    "df_green = df_green \\\n",
    "    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n",
    "\n",
    "df_green.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8dca1a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(VendorID=1, pickup_datetime=datetime.datetime(2020, 1, 1, 19, 45, 57), dropoff_datetime=datetime.datetime(2020, 1, 1, 20, 6, 13), passenger_count=1, trip_distance=6.1, RatecodeID=1, store_and_fwd_flag='N', PULocationID=162, DOLocationID=87, payment_type=1, fare_amount=21.0, extra=2.5, mta_tax=0.5, tip_amount=4.85, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=29.15, congestion_surcharge=2.5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in all yellow data into a Spark DataFrame\n",
    "df_yellow = spark.read.parquet('./data/parquet/yellow/*/*')\n",
    "\n",
    "# Rename some columns via PySpark function to match up with the green taxi data above\n",
    "df_yellow = df_yellow \\\n",
    "    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n",
    "\n",
    "df_yellow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab5ebae",
   "metadata": {},
   "source": [
    "#### We want to eventually combine these two datasets into one large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d4abd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304517\n",
      "39649199\n"
     ]
    }
   ],
   "source": [
    "print(df_green.count())\n",
    "print(df_yellow.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8e41244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PULocationID', 'fare_amount', 'total_amount', 'VendorID', 'RatecodeID', 'DOLocationID', 'tip_amount', 'tolls_amount', 'store_and_fwd_flag', 'trip_distance', 'dropoff_datetime', 'payment_type', 'mta_tax', 'congestion_surcharge', 'pickup_datetime', 'improvement_surcharge', 'extra', 'passenger_count'}\n"
     ]
    }
   ],
   "source": [
    "# get the set of unique columns from the yellow data\n",
    "yellow_columns = set(df_yellow.columns)\n",
    "print(yellow_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9dfc269b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VendorID', 'pickup_datetime', 'dropoff_datetime', 'store_and_fwd_flag', 'RatecodeID', 'PULocationID', 'DOLocationID', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'payment_type', 'congestion_surcharge']\n"
     ]
    }
   ],
   "source": [
    "# Create a list to hold all common columns between the two datasets\n",
    "common_columns = []\n",
    "\n",
    "# Get all columns from the yellow dataset that are also in the green dataset\n",
    "#     and preserve order of cols\n",
    "for col in df_green.columns:\n",
    "    if col in yellow_columns:\n",
    "        common_columns.append(col)\n",
    "        \n",
    "print(common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be2241d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the built-in PySpark functions\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6906571",
   "metadata": {},
   "source": [
    "**`.withColumn()` is a Transformation function of a Spark DataFrame which is used to change the value, convert the datatype of an existing column, create a new column, and many more**\n",
    "- https://sparkbyexamples.com/pyspark/pyspark-withcolumn/\n",
    "\n",
    "**`pyspark.sql.functions.lit` is used to add a new column to DataFrame by assigning a literal or constant value**\n",
    "- https://sparkbyexamples.com/pyspark/pyspark-lit-add-literal-constant/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05e0c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all columns from green that are a part of the common columns\n",
    "#   and then add a new service_type column with value of 'green'\n",
    "df_green_sel = df_green \\\n",
    "    .select(common_columns) \\\n",
    "    .withColumn('service_type', F.lit('green'))\n",
    "\n",
    "# Get all columns from yellow that are a part of the common columns\n",
    "#   and then add a new service_type column with value of 'yellow'\n",
    "df_yellow_sel = df_yellow \\\n",
    "    .select(common_columns) \\\n",
    "    .withColumn('service_type', F.lit('yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "952d3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union the datasets together\n",
    "df_trips_data = df_green_sel.unionAll(df_yellow_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4cb63532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|service_type|   count|\n",
      "+------------+--------+\n",
      "|       green| 2304517|\n",
      "|      yellow|39649199|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the data\n",
    "df_trips_data.groupBy('service_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76e442bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'congestion_surcharge',\n",
       " 'service_type']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the columns\n",
    "df_trips_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7cf247",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "**In order to use Spark SQL, we have to tell Spark that our DataFrame is a table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63610008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell Spark that our DataFrame is a table\n",
    "# df_trips_data.registerTempTable('trips_data')  # deprecated\n",
    "df_trips_data.createOrReplaceTempView('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "469f56e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|service_type|   count|\n",
      "+------------+--------+\n",
      "|       green| 2304517|\n",
      "|      yellow|39649199|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do some Spark SQL on the data\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        service_type,\n",
    "        count(1) AS count\n",
    "    FROM\n",
    "        trips_data\n",
    "    GROUP BY \n",
    "        service_type\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64fdf25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a SQL result to a NEW DataFrame, similar to our dbt models\n",
    "\n",
    "# Get all revenue for each pickup zone/revenue location, for each service type, for each month\n",
    "df_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        -- Reveneue grouping \n",
    "        PULocationID AS revenue_zone,\n",
    "        date_trunc('month', pickup_datetime) AS revenue_month, \n",
    "        service_type, \n",
    "\n",
    "        -- Revenue calculation \n",
    "        SUM(fare_amount) AS revenue_monthly_fare,\n",
    "        SUM(extra) AS revenue_monthly_extra,\n",
    "        SUM(mta_tax) AS revenue_monthly_mta_tax,\n",
    "        SUM(tip_amount) AS revenue_monthly_tip_amount,\n",
    "        SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n",
    "        SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n",
    "        SUM(total_amount) AS revenue_monthly_total_amount,\n",
    "        SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n",
    "\n",
    "        -- Additional calculations\n",
    "        AVG(passenger_count) AS avg_montly_passenger_count,\n",
    "        AVG(trip_distance) AS avg_montly_trip_distance\n",
    "    FROM\n",
    "        trips_data\n",
    "    GROUP BY\n",
    "        1, 2, 3\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3cfae5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+--------------------------+------------------------+\n",
      "|revenue_zone|      revenue_month|service_type|revenue_monthly_fare|revenue_monthly_extra|revenue_monthly_mta_tax|revenue_monthly_tip_amount|revenue_monthly_tolls_amount|revenue_monthly_improvement_surcharge|revenue_monthly_total_amount|revenue_monthly_congestion_surcharge|avg_montly_passenger_count|avg_montly_trip_distance|\n",
      "+------------+-------------------+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+--------------------------+------------------------+\n",
      "|         218|2020-01-01 00:00:00|       green|  24689.350000000126|              1561.75|                  121.5|                      18.7|           735.9000000000004|                    246.0000000000036|          27375.149999999958|                                 0.0|        1.0754716981132075|       6.732052451539335|\n",
      "|          85|2020-01-01 00:00:00|       green|   19874.88000000006|               1476.5|                  187.5|        165.89999999999998|           354.6800000000002|                   233.70000000000314|            22320.9099999999|                                11.0|        1.3349753694581281|      4.4563455149501605|\n",
      "|         116|2019-12-01 00:00:00|       green|               21.59|                  1.0|                    1.0|                      4.95|                         0.0|                   0.8999999999999999|          29.439999999999998|                                 0.0|                       1.0|      0.6366666666666666|\n",
      "|         112|2020-01-01 00:00:00|       green|  21761.030000000017|               1034.0|                  703.0|        2487.4200000000014|           308.6100000000001|                    452.7000000000114|           27470.10999999962|                              742.25|        1.2766407904022583|      3.4783365695792865|\n",
      "|          39|2020-01-01 00:00:00|       green|   60305.34999999994|               3876.0|                  491.0|                     29.63|          1069.9199999999996|                    561.0000000000063|           66334.85000000037|                                 0.0|        1.2745098039215685|       6.203130280124502|\n",
      "+------------+-------------------+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+--------------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3778cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(revenue_zone=218, revenue_month=datetime.datetime(2020, 1, 1, 0, 0), service_type='green', revenue_monthly_fare=24689.350000000126, revenue_monthly_extra=1561.75, revenue_monthly_mta_tax=121.5, revenue_monthly_tip_amount=18.7, revenue_monthly_tolls_amount=735.9000000000004, revenue_monthly_improvement_surcharge=246.0000000000036, revenue_monthly_total_amount=27375.149999999958, revenue_monthly_congestion_surcharge=0.0, avg_montly_passenger_count=1.0754716981132075, avg_montly_trip_distance=6.732052451539335),\n",
       " Row(revenue_zone=85, revenue_month=datetime.datetime(2020, 1, 1, 0, 0), service_type='green', revenue_monthly_fare=19874.88000000006, revenue_monthly_extra=1476.5, revenue_monthly_mta_tax=187.5, revenue_monthly_tip_amount=165.89999999999998, revenue_monthly_tolls_amount=354.6800000000002, revenue_monthly_improvement_surcharge=233.70000000000314, revenue_monthly_total_amount=22320.9099999999, revenue_monthly_congestion_surcharge=11.0, avg_montly_passenger_count=1.3349753694581281, avg_montly_trip_distance=4.4563455149501605),\n",
       " Row(revenue_zone=116, revenue_month=datetime.datetime(2019, 12, 1, 0, 0), service_type='green', revenue_monthly_fare=21.59, revenue_monthly_extra=1.0, revenue_monthly_mta_tax=1.0, revenue_monthly_tip_amount=4.95, revenue_monthly_tolls_amount=0.0, revenue_monthly_improvement_surcharge=0.8999999999999999, revenue_monthly_total_amount=29.439999999999998, revenue_monthly_congestion_surcharge=0.0, avg_montly_passenger_count=1.0, avg_montly_trip_distance=0.6366666666666666),\n",
       " Row(revenue_zone=112, revenue_month=datetime.datetime(2020, 1, 1, 0, 0), service_type='green', revenue_monthly_fare=21761.030000000017, revenue_monthly_extra=1034.0, revenue_monthly_mta_tax=703.0, revenue_monthly_tip_amount=2487.4200000000014, revenue_monthly_tolls_amount=308.6100000000001, revenue_monthly_improvement_surcharge=452.7000000000114, revenue_monthly_total_amount=27470.10999999962, revenue_monthly_congestion_surcharge=742.25, avg_montly_passenger_count=1.2766407904022583, avg_montly_trip_distance=3.4783365695792865),\n",
       " Row(revenue_zone=39, revenue_month=datetime.datetime(2020, 1, 1, 0, 0), service_type='green', revenue_monthly_fare=60305.34999999994, revenue_monthly_extra=3876.0, revenue_monthly_mta_tax=491.0, revenue_monthly_tip_amount=29.63, revenue_monthly_tolls_amount=1069.9199999999996, revenue_monthly_improvement_surcharge=561.0000000000063, revenue_monthly_total_amount=66334.85000000037, revenue_monthly_congestion_surcharge=0.0, avg_montly_passenger_count=1.2745098039215685, avg_montly_trip_distance=6.203130280124502)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705faae",
   "metadata": {},
   "source": [
    "**`.coalesce(*cols)` returns the first column that is not null and is *used to decrease the number of partitions in an efficient way***\n",
    "\n",
    "**Can go to the Spark UI after running the next cell to see jobs/tasks running**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c4933766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use just 1 partition via .coalesce()\n",
    "df_result.coalesce(1) \\\n",
    "    .write.parquet('data/report/revenue/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7f1ffcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 08A3-CF2D\n",
      "\n",
      " Directory of C:\\Users\\nimz\\Documents\\de_zoomcamp\\week5_batch_processing\\data\\report\\revenue\n",
      "\n",
      "02/27/2024  09:11 PM    <DIR>          .\n",
      "02/27/2024  09:11 PM    <DIR>          ..\n",
      "02/27/2024  09:11 PM             4,112 .part-00000-596e3dca-5394-4d49-9d44-c8d07b1cacb2-c000.snappy.parquet.crc\n",
      "02/27/2024  09:11 PM                 8 ._SUCCESS.crc\n",
      "02/27/2024  09:11 PM           524,936 part-00000-596e3dca-5394-4d49-9d44-c8d07b1cacb2-c000.snappy.parquet\n",
      "02/27/2024  09:11 PM                 0 _SUCCESS\n",
      "               4 File(s)        529,056 bytes\n",
      "               2 Dir(s)  240,145,195,008 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir data\\report\\revenue\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-zoom]",
   "language": "python",
   "name": "conda-env-.conda-zoom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
